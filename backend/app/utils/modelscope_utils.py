import torch
import os
import gc
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from backend.app.utils.logger import logger
from anyio import to_thread

# å°è¯•å¯¼å…¥ modelscope ç›¸å…³åº“ (å¯é€‰ä¾èµ–)
try:
    # ä¼˜å…ˆå°è¯•ä» transformers å¯¼å…¥æ¨¡å‹ç±» (æ›´é€šç”¨)
    from transformers import Qwen2_5_VLForConditionalGeneration, Qwen3VLForConditionalGeneration, AutoProcessor
    from qwen_vl_utils import process_vision_info
    _MODELSCOPE_AVAILABLE = True
except ImportError:
    try:
        # Fallback if Qwen3 is not available
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        from qwen_vl_utils import process_vision_info
        Qwen3VLForConditionalGeneration = None
        _MODELSCOPE_AVAILABLE = True
    except ImportError:
        _MODELSCOPE_AVAILABLE = False
    logger.warning("âš ï¸ transformers æˆ– qwen_vl_utils æœªå®‰è£…ï¼ŒModelScopeUtils åŠŸèƒ½å—é™")

class ModelScopeUtils:
    """
    ModelScope æ¨¡å‹é€šç”¨å·¥å…·ç±» (ç®¡ç†æœ¬åœ° ModelScope æ¨¡å‹åŠ è½½ä¸æ¨ç†)
    æ”¯æŒ: 
    - å¤šGPUè‡ªåŠ¨é€‰æ‹© (æŒ‰æ˜¾å­˜ç©ºé—²)
    - æ˜¾å­˜è‡ªåŠ¨å¸è½½ (é¿å…OOM)
    - å¼‚æ­¥é˜Ÿåˆ—é” (é˜²æ­¢æ¨ç†å†²çª)
    """
    _instances = {} # ç¼“å­˜ä¸åŒæ¨¡å‹çš„å®ä¾‹ (model_name -> {"model": ..., "processor": ..., "device": ...})
    _inference_lock = asyncio.Lock() # å…¨å±€æ¨ç†é”ï¼Œé˜²æ­¢å¹¶å‘æ¨ç†å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸æˆ–æ¨¡å‹åˆ‡æ¢å†²çª
    
    # é»˜è®¤æ¨¡å‹è·¯å¾„æ˜ å°„ (å¯æ‰©å±•)
    # æ ¼å¼: "ShortName": "Relative/Path/To/Model"
    _MODEL_PATHS = {
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct",
        "Qwen3-VL-8B-Instruct": "Qwen/Qwen3-VL-8B-Instruct",
        "Qwen/Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct",
        "Qwen/Qwen3-VL-8B-Instruct": "Qwen/Qwen3-VL-8B-Instruct"
    }

    @classmethod
    def get_model_path(cls, model_name: str) -> str:
        """
        è·å–æ¨¡å‹ç»å¯¹è·¯å¾„ (æ”¯æŒè‡ªåŠ¨å‘ç°)
        """
        base_path = Path(__file__).parent.parent.parent / "app" / "models"
        
        # 1. æŸ¥è¡¨
        relative_path = cls._MODEL_PATHS.get(model_name)
        
        # 2. å¦‚æœè¡¨ä¸­æ²¡æœ‰ï¼Œå°è¯•è‡ªåŠ¨å‘ç°
        if not relative_path:
            # å¦‚æœä¼ å…¥çš„æ˜¯ namespace/model_name æ ¼å¼ï¼Œç›´æ¥å°è¯•æ‹¼æ¥
            if "/" in model_name:
                parts = model_name.split("/")
                if len(parts) >= 2:
                    potential_path = base_path / parts[0] / parts[1]
                    if potential_path.exists():
                        return str(potential_path)

            logger.info(f"æ­£åœ¨è‡ªåŠ¨æ‰«ææŸ¥æ‰¾æ¨¡å‹: {model_name} ...")
            relative_path = cls._scan_and_find_model(model_name)
            if relative_path:
                logger.success(f"å·²è‡ªåŠ¨å®šä½æ¨¡å‹è·¯å¾„: {relative_path}")
                # ç¼“å­˜ç»“æœ
                cls._MODEL_PATHS[model_name] = relative_path
        
        if not relative_path:
            # å¦‚æœæ˜¯ full id ä¸”ä¸å­˜åœ¨ï¼Œè¿”å›é¢„æœŸçš„è·¯å¾„ä»¥ä¾¿åç»­ä¸‹è½½
            if "/" in model_name:
                 return str(base_path / model_name)
            return ""
            
        return str(base_path / relative_path)
    
    @classmethod
    def _scan_and_find_model(cls, model_name: str) -> Optional[str]:
        """
        æ‰«æ models ç›®å½•æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹è·¯å¾„
        """
        base_path = Path(__file__).parent.parent.parent / "app" / "models"
        if not base_path.exists():
            return None
            
        # 1. ç²¾ç¡®åŒ¹é…ç›®å½•å
        # éå†æ‰€æœ‰å­ç›®å½•å¯»æ‰¾ config.json
        for root, dirs, files in os.walk(base_path):
            if "config.json" in files:
                abs_path = Path(root)
                # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ¹é… model_name
                if abs_path.name.lower() == model_name.lower():
                    # æ‰¾åˆ°åŒ¹é…ï¼Œè®¡ç®—ç›¸å¯¹è·¯å¾„
                    rel_path = abs_path.relative_to(base_path)
                    return str(rel_path).replace("\\", "/")
                    
        return None
        
    @classmethod
    def check_model_exists(cls, model_name: str = "Qwen/Qwen3-VL-4B-Instruct") -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        path_str = cls.get_model_path(model_name)
        if not path_str:
            return False
        path = Path(path_str)
        return path.exists() and (path / "config.json").exists()

    @classmethod
    def _get_best_device(cls) -> str:
        """
        è·å–æœ€ä½³è®¡ç®—è®¾å¤‡ (ä¼˜å…ˆé€‰æ‹©ç©ºé—²æ˜¾å­˜æœ€å¤§çš„ GPU)
        """
        if not torch.cuda.is_available():
            return "cpu"
        
        try:
            device_count = torch.cuda.device_count()
            if device_count == 1:
                return "cuda:0"
            
            # å¤šå¡é€‰æ‹©ï¼šé€‰æ‹©å‰©ä½™æ˜¾å­˜æœ€å¤§çš„å¡
            max_free_memory = 0
            best_device_idx = 0
            
            for i in range(device_count):
                free_memory = torch.cuda.mem_get_info(i)[0]
                if free_memory > max_free_memory:
                    max_free_memory = free_memory
                    best_device_idx = i
            
            device_str = f"cuda:{best_device_idx}"
            logger.info(f"âš¡ [GPUé€‰æ‹©] è‡ªåŠ¨é€‰æ‹©æ˜¾å­˜æœ€å……è¶³è®¾å¤‡: {device_str} (å‰©ä½™: {max_free_memory / 1024**3:.2f} GB)")
            return device_str
            
        except Exception as e:
            logger.warning(f"è·å–æœ€ä½³è®¾å¤‡å¤±è´¥ï¼Œå›é€€åˆ° cuda:0: {e}")
            return "cuda:0"
    
    @classmethod
    def unload_model(cls, model_name: str):
        """
        å¸è½½æŒ‡å®šæ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
        """
        if model_name in cls._instances:
            logger.info(f"ğŸ§¹ æ­£åœ¨å¸è½½æ¨¡å‹: {model_name}")
            del cls._instances[model_name]
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.success(f"æ¨¡å‹ {model_name} å·²å¸è½½ï¼Œæ˜¾å­˜å·²æ¸…ç†")
    
    @classmethod
    def _load_model(cls, model_name: str):
        """
        åŠ è½½æŒ‡å®šæ¨¡å‹ (æ‡’åŠ è½½ + è‡ªåŠ¨æ˜¾å­˜ç®¡ç†)
        æ³¨æ„ï¼šå¿…é¡»åœ¨ _inference_lock ä¿æŠ¤ä¸‹è°ƒç”¨
        """
        if not _MODELSCOPE_AVAILABLE:
            raise RuntimeError("è¯·å…ˆå®‰è£… modelscope: pip install modelscope qwen-vl-utils")

        if model_name in cls._instances:
            return cls._instances[model_name]

        model_path = cls.get_model_path(model_name)
        if not cls.check_model_exists(model_name):
             # è‡ªåŠ¨ä¸‹è½½
             logger.info(f"ğŸ“¥ ModelScope æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½: {model_name} -> {cls.BASE_MODEL_DIR}")
             try:
                 from modelscope.hub.snapshot_download import snapshot_download
                 # ä¸‹è½½åˆ° backend/app/models
                 snapshot_download(model_name, cache_dir=str(cls.BASE_MODEL_DIR))
                 logger.success(f"âœ… [{model_name}] æ¨¡å‹ä¸‹è½½å®Œæˆ")
                 
                 # é‡æ–°è·å–è·¯å¾„ (ä»¥é˜²ä¸‡ä¸€)
                 model_path = cls.get_model_path(model_name)
             except Exception as e:
                 logger.error(f"âŒ [{model_name}] æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
                 raise e

        try:
            # ç­–ç•¥ï¼šå¦‚æœå·²åŠ è½½å…¶ä»–æ¨¡å‹ï¼Œå…ˆå¸è½½ä»¥é‡Šæ”¾æ˜¾å­˜ (å•å¡/èµ„æºå—é™åœºæ™¯)
            if cls._instances:
                logger.warning(f"âš ï¸ èµ„æºå—é™: æ­£åœ¨å¸è½½å…¶ä»–æ¨¡å‹ä»¥åŠ è½½ {model_name}...")
                for name in list(cls._instances.keys()):
                    # å¦‚æœéœ€è¦åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹ï¼Œè¿™é‡Œéœ€è¦æ›´å¤æ‚çš„ç­–ç•¥
                    cls.unload_model(name)

            logger.info(f"æ­£åœ¨åŠ è½½ ModelScope æ¨¡å‹ [{model_name}]: {model_path}")
            
            # æ™ºèƒ½é€‰æ‹©è®¾å¤‡
            device = cls._get_best_device()
            logger.info(f"[{model_name}] ä½¿ç”¨è®¾å¤‡: {device}")

            # æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½
            if "Qwen3-VL" in model_name:
                if Qwen3VLForConditionalGeneration is None:
                     raise ImportError("å½“å‰ transformers ç‰ˆæœ¬ä¸æ”¯æŒ Qwen3-VL")
                model_class = Qwen3VLForConditionalGeneration
            elif "Qwen2.5-VL" in model_name or "Qwen2-VL" in model_name:
                model_class = Qwen2_5_VLForConditionalGeneration
            else:
                 # Default fallback or error
                 raise NotImplementedError(f"å°šæœªæ”¯æŒè¯¥æ¨¡å‹ç±»å‹çš„åŠ è½½: {model_name}")

            # ä½¿ç”¨ AutoModel è‡ªåŠ¨é€‚é… Qwen2/2.5/3 VL
            model = model_class.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16 if "cuda" in device else torch.float32,
                # trust_remote_code=True, # å…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
                ignore_mismatched_sizes=True,  # å…è®¸å¿½ç•¥æƒé‡å½¢çŠ¶ä¸åŒ¹é… (å¦‚å¾®è°ƒå¤´å·®å¼‚)
            ).to(device)
            processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
            
            cls._instances[model_name] = {
                "model": model,
                "processor": processor,
                "device": device
            }
            logger.success(f"[{model_name}] åŠ è½½æˆåŠŸ!")
            return cls._instances[model_name]
            
        except Exception as e:
            logger.error(f"[{model_name}] åŠ è½½å¤±è´¥: {e}")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise RuntimeError(f"Failed to load model {model_name}: {e}")

    @classmethod
    def _run_inference_sync(cls, model_name: str, messages: List[Dict[str, Any]], max_new_tokens: int, streamer=None) -> str:
        """
        åŒæ­¥æ‰§è¡Œæ¨ç†é€»è¾‘ (å°†è¢«è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­)
        """
        instance = cls._load_model(model_name)
        model = instance["model"]
        processor = instance["processor"]
        device = instance["device"]
        
        # Qwen-VL ç‰¹æœ‰å¤„ç†é€»è¾‘
        if "Qwen" in model_name:
            # 1. åº”ç”¨èŠå¤©æ¨¡æ¿
            text = processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # 2. å¤„ç†è§†è§‰ä¿¡æ¯
            image_inputs, video_inputs = process_vision_info(messages)
            
            # 3. ç¼–ç è¾“å…¥
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            
            # ç§»è‡³è®¾å¤‡
            inputs = inputs.to(device)

            # Debug Log: Print Input Shapes
            logger.info(f"[{model_name}] Input Keys: {list(inputs.keys())}")
            if "pixel_values" in inputs:
                logger.info(f"[{model_name}] pixel_values shape: {inputs['pixel_values'].shape}")
            if "image_grid_thw" in inputs:
                logger.info(f"[{model_name}] image_grid_thw: {inputs['image_grid_thw']}")
            if "input_ids" in inputs:
                logger.info(f"[{model_name}] input_ids shape: {inputs['input_ids'].shape}")

            # 4. ç”Ÿæˆ
            logger.info(f"[{model_name}] å¼€å§‹æ¨ç†...")
            if streamer:
                # ä½¿ç”¨ streamer è¿›è¡Œæµå¼ç”Ÿæˆ
                model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)
                return "" # æµå¼æ¨¡å¼ä¸‹è¿”å›å€¼ç”± streamer å¤„ç†ï¼Œè¿™é‡Œè¿”å›ç©ºæˆ–æœ€åç´¯ç§¯çš„æ–‡æœ¬
            else:
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)
                
                # 5. è§£ç 
                generated_ids_trimmed = [
                    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )
                
                result = output_text[0]
                logger.info(f"[{model_name}] æ¨ç†å®Œæˆ: {result[:50]}...")
                return result
            
        return "Unsupported model architecture"

    @classmethod
    async def chat_completion_stream(
        cls, 
        messages: List[Dict[str, Any]], 
        model_name: str = "Qwen/Qwen3-VL-4B-Instruct",
        max_new_tokens: int = 512
    ):
        """
        æ‰§è¡Œå¯¹è¯æ¨ç† (å¼‚æ­¥æµå¼)
        """
        from transformers import TextIteratorStreamer
        import threading

        # åŠ è½½æ¨¡å‹ (è·å– processor)
        # æ³¨æ„: è¿™é‡Œéœ€è¦åœ¨ä¸»çº¿ç¨‹åŠ è½½ï¼Œå› ä¸º load_model å¯èƒ½æ¶‰åŠä¸‹è½½å’Œ GPU æ“ä½œ
        async with cls._inference_lock:
            instance = cls._load_model(model_name)
            processor = instance["processor"]
            
            # åˆ›å»º Streamer
            streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œ generate
            # æ³¨æ„: generate æ˜¯é˜»å¡çš„ï¼Œå¿…é¡»åœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼Œå¦åˆ™ä¼šé˜»å¡ event loop å¯¼è‡´æ— æ³• yield
            thread = threading.Thread(
                target=cls._run_inference_sync, 
                kwargs={
                    "model_name": model_name,
                    "messages": messages,
                    "max_new_tokens": max_new_tokens,
                    "streamer": streamer
                }
            )
            thread.start()

            # åœ¨ä¸»çº¿ç¨‹ä¸­ yield streamer çš„è¾“å‡º
            # streamer æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œä¼šé˜»å¡ç­‰å¾…æ–° token
            try:
                for new_text in streamer:
                    yield new_text
            except Exception as e:
                logger.error(f"æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
                yield f"[ERROR: {str(e)}]"
            finally:
                thread.join()

    @classmethod
    async def chat_completion(
        cls, 
        messages: List[Dict[str, Any]], 
        model_name: str = "Qwen/Qwen3-VL-4B-Instruct",
        max_new_tokens: int = 512
    ) -> str:
        """
        æ‰§è¡Œå¯¹è¯æ¨ç† (å¼‚æ­¥é˜Ÿåˆ— + çº¿ç¨‹æ± )
        """
        # ä½¿ç”¨é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨¡å‹æ“ä½œåœ¨è¿›è¡Œ (é˜²æ­¢æ¨¡å‹åˆ‡æ¢å†²çª)
        async with cls._inference_lock:
            try:
                # å°† CPU å¯†é›†å‹çš„åŠ è½½å’Œæ¨ç†ä»»åŠ¡æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                result = await to_thread.run_sync(
                    cls._run_inference_sync,
                    model_name,
                    messages,
                    max_new_tokens,
                    None # No streamer
                )
                return result
                
            except torch.cuda.OutOfMemoryError:
                logger.error(f"æ˜¾å­˜ä¸è¶³ (OOM) æ‰§è¡Œæ¨¡å‹: {model_name}")
                cls.unload_model(model_name)
                raise RuntimeError("GPU Out of Memory. Please try again later.")
            except Exception as e:
                logger.error(f"æ¨ç†è¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                raise e