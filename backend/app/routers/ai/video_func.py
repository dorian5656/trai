#!/usr/bin/env python
# -*- coding: utf-8 -*-
# æ–‡ä»¶åï¼šbackend/app/routers/ai/video_func.py
# ä½œè€…ï¼šliuhd
# æ—¥æœŸï¼š2026-02-06
# æè¿°ï¼šAI è§†é¢‘ç”Ÿæˆä¸šåŠ¡é€»è¾‘ (Wan2.1-T2V-1.3B)

import os
import sys
import uuid
import time
import torch

import random
import numpy as np
from pathlib import Path
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from easydict import EasyDict
from safetensors.torch import load_file
from contextlib import asynccontextmanager

from PIL import Image
import torchvision.transforms as transforms

# é¡¹ç›®å†…éƒ¨å¼•ç”¨
from backend.app.config import settings
from backend.app.utils.logger import logger
from backend.app.utils.pg_utils import PGUtils
from backend.app.utils.upload_utils import UploadUtils
from backend.app.utils.feishu_utils import FeishuBot
from backend.app.utils.ai_utils import AIUtils

# Wan-AI æ¨¡å—å¼•ç”¨
# å‡è®¾ Wan æ¨¡å—åœ¨ backend/app/engines/Robbyant/lingbot ç›®å½•ä¸‹
WAN_MODULE_PATH = Path("/home/code_dev/trai/backend/app/engines/Robbyant/lingbot")
if str(WAN_MODULE_PATH) not in sys.path:
    sys.path.append(str(WAN_MODULE_PATH))

try:
    from wan.modules.model import WanModel
    from wan.modules.t5 import T5EncoderModel
    from wan.modules.vae2_1 import Wan2_1_VAE
    from wan.modules.vae2_2 import Wan2_2_VAE
    from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler
    from wan.utils.utils import save_video
except ImportError as e:
    logger.error(f"Wan-AI module import failed: {e}")

# =============================================================================
# Schema å®šä¹‰
# =============================================================================

class VideoGenRequest(BaseModel):
    """
    æ–‡ç”Ÿè§†é¢‘è¯·æ±‚
    """
    prompt: str = Field(..., description="æç¤ºè¯", examples=["ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨è‰åœ°ä¸Šå¥”è·‘"])
    model: str = Field("Wan2.1-T2V-1.3B", description="æ¨¡å‹åç§°", examples=["Wan2.1-T2V-1.3B"])
    ratio: str = Field("16:9", description="å®½é«˜æ¯”", examples=["16:9"])
    duration: int = Field(5, description="è§†é¢‘æ—¶é•¿(ç§’) - å®é™…ä¸Šç”± frame_num å†³å®š, è¿™é‡Œä»…ä½œå‚è€ƒ", examples=[5])
    sampling_steps: int = Field(20, description="é‡‡æ ·æ­¥æ•°", examples=[20])
    guide_scale: float = Field(5.0, description="å¼•å¯¼ç³»æ•°", examples=[5.0])
    seed: int = Field(-1, description="éšæœºç§å­ (-1 è¡¨ç¤ºéšæœº)", examples=[-1])
    image_path: Optional[str] = Field(None, description="å›¾ç”Ÿè§†é¢‘çš„å‚è€ƒå›¾ç‰‡è·¯å¾„ (æœ¬åœ°ç»å¯¹è·¯å¾„)", examples=["/home/user/test.png"])

class VideoGenResponse(BaseModel):
    """
    æ–‡ç”Ÿè§†é¢‘å“åº”
    """
    video_url: str = Field(..., description="è§†é¢‘ URL")
    cover_url: Optional[str] = Field(None, description="å°é¢å›¾ URL")
    cost_time: float = Field(..., description="è€—æ—¶(ç§’)")

# =============================================================================
# WanT2V å°è£…ç±»
# =============================================================================

class WanT2VWrapper:
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(WanT2VWrapper, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def __init__(self):
        if self.initialized:
            return
            
        # è‡ªåŠ¨é€‰æ‹©æ˜¾å­˜æœ€ç©ºé—²çš„ GPU
        best_gpu = AIUtils.get_best_gpu_device()
        self.device = torch.device(best_gpu)
        logger.info(f"WanT2VWrapper ç»‘å®šè®¾å¤‡: {self.device}")
        
        # é»˜è®¤æ¨¡å‹è·¯å¾„
        self.base_model_dir = '/home/code_dev/trai/backend/app/models/Wan-AI'
        self.checkpoint_dir = os.path.join(self.base_model_dir, 'Wan2.1-T2V-1.3B')
        self.current_model_name = "Wan2.1-T2V-1.3B"
        
        self.init_config()
        
        self.text_encoder = None
        self.vae = None
        self.model = None
        
        self.initialized = True
        logger.info("WanT2VWrapper é…ç½®åˆå§‹åŒ–å®Œæˆã€‚")

    def init_config(self, model_name: str = "Wan2.1-T2V-1.3B"):
        """
        åˆå§‹åŒ–é…ç½®ï¼Œæ ¹æ®æ¨¡å‹åç§°åŠ è½½ä¸åŒçš„å‚æ•°
        """
        self.current_model_name = model_name
        self.checkpoint_dir = os.path.join(self.base_model_dir, model_name)
        
        logger.info(f"Initializing config for model: {model_name} at {self.checkpoint_dir}")
        
        self.config = EasyDict()
        
        # é€šç”¨é…ç½®
        self.config.t5_model = 'umt5_xxl'
        self.config.t5_dtype = torch.bfloat16
        self.config.text_len = 512
        self.config.param_dtype = torch.bfloat16
        self.config.num_train_timesteps = 1000
        self.config.sample_fps = 16
        self.config.sample_neg_prompt = 'è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°'
        self.config.frame_num = 81
        self.config.t5_checkpoint = 'models_t5_umt5-xxl-enc-bf16.pth'
        self.config.t5_tokenizer = 'google/umt5-xxl'
        self.config.vae_checkpoint = 'Wan2.1_VAE.pth'
        self.config.vae_stride = (4, 8, 8)
        self.config.patch_size = (1, 2, 2)
        self.config.window_size = (-1, -1)
        self.config.qk_norm = True
        self.config.cross_attn_norm = True
        self.config.eps = 1e-6
        self.config.in_dim = 16
        self.config.text_dim = 4096
        self.config.out_dim = 16

        # æ¨¡å‹ç‰¹å®šé…ç½®
        if "1.3B" in model_name:
            self.config.dim = 1536
            self.config.ffn_dim = 8960
            self.config.freq_dim = 256
            self.config.num_heads = 12
            self.config.num_layers = 30
        elif "5B" in model_name: # Wan2___2-TI2V-5B
            # æ ¹æ® config.json å†…å®¹
            # "dim": 3072, "ffn_dim": 14336, "num_heads": 24, "num_layers": 30, "in_dim": 48, "out_dim": 48
            self.config.dim = 3072
            self.config.ffn_dim = 14336
            self.config.freq_dim = 256
            self.config.num_heads = 24
            self.config.num_layers = 30
            self.config.in_dim = 48
            self.config.out_dim = 48
            self.config.vae_checkpoint = 'Wan2.2_VAE.pth'
            self.config.vae_stride = (4, 16, 16)
            # æ³¨æ„ï¼šç”¨æˆ·ä½¿ç”¨çš„æ˜¯ TI2V æ¨¡å‹è¿›è¡Œ T2V ä»»åŠ¡ï¼Œè¿™é‡Œæˆ‘ä»¬å°†é€šé“æ•°è°ƒæ•´ä¸º 48 ä»¥åŒ¹é…æƒé‡
            # ä½†å®é™…ç”Ÿæˆæ•ˆæœå¯èƒ½å—é™ï¼Œå› ä¸ºç¼ºå¤±äº†å›¾åƒæ¡ä»¶


    def load_models(self, model_name: str = "Wan2.1-T2V-1.3B"):
        """
        åŠ è½½æ¨¡å‹ (Lazy Loading)
        """
        # å¦‚æœè¯·æ±‚çš„æ¨¡å‹å’Œå½“å‰åŠ è½½çš„ä¸ä¸€è‡´ï¼Œéœ€è¦é‡æ–°åŠ è½½
        if self.model is not None and self.current_model_name != model_name:
            logger.info(f"Switching model from {self.current_model_name} to {model_name}...")
            # é‡Šæ”¾æ—§æ¨¡å‹
            del self.model
            del self.text_encoder
            del self.vae
            torch.cuda.empty_cache()
            self.model = None
            self.text_encoder = None
            self.vae = None
            
        if self.model is not None:
            return

        # æ›´æ–°é…ç½®
        self.init_config(model_name)

        logger.info(f"æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
        try:
            # 1. T5 Encoder
            # T5 and VAE are often shared or located in the 1.3B directory if not present in the current model dir
            # Fallback to 1.3B directory for common components if file missing
            t5_ckpt_path = os.path.join(self.checkpoint_dir, self.config.t5_checkpoint)
            t5_tokenizer_path = os.path.join(self.checkpoint_dir, self.config.t5_tokenizer)
            
            # Fallback logic
            default_model_dir = os.path.join(self.base_model_dir, 'Wan2.1-T2V-1.3B')
            if not os.path.exists(t5_ckpt_path):
                t5_ckpt_path = os.path.join(default_model_dir, self.config.t5_checkpoint)
                logger.warning(f"T5 checkpoint not found in {self.checkpoint_dir}, falling back to {t5_ckpt_path}")
                
            if not os.path.exists(t5_tokenizer_path):
                t5_tokenizer_path = os.path.join(default_model_dir, self.config.t5_tokenizer)
                logger.warning(f"T5 tokenizer not found in {self.checkpoint_dir}, falling back to {t5_tokenizer_path}")

            self.text_encoder = T5EncoderModel(
                text_len=self.config.text_len,
                dtype=self.config.t5_dtype,
                device=torch.device('cpu'), # Init on CPU
                checkpoint_path=t5_ckpt_path,
                tokenizer_path=t5_tokenizer_path,
                shard_fn=None,
            )

            # 2. VAE
            vae_ckpt_path = os.path.join(self.checkpoint_dir, self.config.vae_checkpoint)
            if not os.path.exists(vae_ckpt_path):
                vae_ckpt_path = os.path.join(default_model_dir, self.config.vae_checkpoint)
                logger.warning(f"VAE checkpoint not found in {self.checkpoint_dir}, falling back to {vae_ckpt_path}")

            if os.path.basename(vae_ckpt_path) == 'Wan2.2_VAE.pth':
                self.vae = Wan2_2_VAE(
                    vae_pth=vae_ckpt_path,
                    device=self.device,
                    dtype=self.config.param_dtype)
            else:
                self.vae = Wan2_1_VAE(
                    vae_pth=vae_ckpt_path,
                    device=self.device,
                    dtype=self.config.param_dtype)

            # 3. WanModel
            # Determine model type from config or name
            model_type = 't2v'
            if 'TI2V' in self.current_model_name:
                model_type = 'ti2v'
                # TI2V specific adjustments if needed
                self.config.in_dim = 48
                self.config.out_dim = 48
            
            self.model = WanModel(
                model_type=model_type,
                patch_size=self.config.patch_size,
                text_len=self.config.text_len,
                in_dim=self.config.in_dim,
                dim=self.config.dim,
                ffn_dim=self.config.ffn_dim,
                freq_dim=self.config.freq_dim,
                text_dim=self.config.text_dim,
                out_dim=self.config.out_dim,
                num_heads=self.config.num_heads,
                num_layers=self.config.num_layers,
                window_size=self.config.window_size,
                qk_norm=self.config.qk_norm,
                cross_attn_norm=self.config.cross_attn_norm,
                eps=self.config.eps
            )
            
            # åŠ è½½æƒé‡
            from safetensors.torch import load_file
            import json

            # æ”¯æŒåˆ†ç‰‡æƒé‡ (safetensors)
            if "5B" in self.current_model_name:
                # 5B æ¨¡å‹é€šå¸¸æœ‰åˆ†ç‰‡æƒé‡
                index_file = os.path.join(self.checkpoint_dir, "diffusion_pytorch_model.safetensors.index.json")
                if os.path.exists(index_file):
                    # åŠ è½½åˆ†ç‰‡æƒé‡
                    with open(index_file, 'r') as f:
                        index_data = json.load(f)
                    
                    weight_map = index_data.get("weight_map", {})
                    # è·å–å”¯ä¸€æ–‡ä»¶åˆ—è¡¨
                    shard_files = set(weight_map.values())
                    
                    for shard_file in shard_files:
                        shard_path = os.path.join(self.checkpoint_dir, shard_file)
                        logger.info(f"åŠ è½½åˆ†ç‰‡: {shard_file}")
                        state_dict = load_file(shard_path)
                        # ä¿®å¤ CUDA å†…å­˜å¯¹é½é”™è¯¯ï¼šç¡®ä¿å¼ é‡å†…å­˜è¿ç»­å¹¶ç»Ÿä¸€æ•°æ®ç±»å‹
                        for k, v in state_dict.items():
                            if isinstance(v, torch.Tensor):
                                state_dict[k] = v.contiguous().to(self.config.param_dtype)
                        self.model.load_state_dict(state_dict, strict=False)
                else:
                    # å¦‚æœç´¢å¼•ä¸¢å¤±ï¼Œå°è¯•å•æ–‡ä»¶å›é€€æˆ–ç‰¹å®šåˆ†ç‰‡ï¼ˆ5Bä¸å¤ªå¯èƒ½ï¼‰
                     # æ‰‹åŠ¨åŠ è½½å·²çŸ¥åˆ†ç‰‡ï¼ˆå¦‚æœç´¢å¼•ä¸¢å¤±ä½†æ–‡ä»¶å­˜åœ¨ï¼‰
                    shard_1 = os.path.join(self.checkpoint_dir, "diffusion_pytorch_model-00001-of-00003.safetensors")
                    if os.path.exists(shard_1):
                        for i in range(1, 4):
                            shard_name = f"diffusion_pytorch_model-0000{i}-of-00003.safetensors"
                            shard_path = os.path.join(self.checkpoint_dir, shard_name)
                            if os.path.exists(shard_path):
                                logger.info(f"åŠ è½½åˆ†ç‰‡: {shard_name}")
                                state_dict = load_file(shard_path)
                                # ä¿®å¤ CUDA å†…å­˜å¯¹é½é”™è¯¯ï¼šç¡®ä¿å¼ é‡å†…å­˜è¿ç»­å¹¶ç»Ÿä¸€æ•°æ®ç±»å‹
                                for k, v in state_dict.items():
                                    if isinstance(v, torch.Tensor):
                                        state_dict[k] = v.contiguous().to(self.config.param_dtype)
                                self.model.load_state_dict(state_dict, strict=False)
                    else:
                        raise FileNotFoundError(f"åœ¨ {self.checkpoint_dir} ä¸­æœªæ‰¾åˆ°æƒé‡")
            else:
                # 1.3B å•æ–‡ä»¶
                state_dict_path = os.path.join(self.checkpoint_dir, "diffusion_pytorch_model.safetensors")
                if os.path.exists(state_dict_path):
                    state_dict = load_file(state_dict_path)
                    # 1.3B åŒæ ·ä¿®å¤å†…å­˜å¯¹é½é”™è¯¯
                    for k, v in state_dict.items():
                        if isinstance(v, torch.Tensor):
                            state_dict[k] = v.contiguous().to(self.config.param_dtype)
                    self.model.load_state_dict(state_dict, strict=False)
                else:
                    raise FileNotFoundError(f"åœ¨ {state_dict_path} æœªæ‰¾åˆ°æƒé‡")

            self.model.eval().requires_grad_(False)
            self.model.to(self.device)
            self.model.to(self.config.param_dtype)
            
            # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°å’ŒBufferï¼Œç¡®ä¿æ— é—æ¼
            for name, param in self.model.named_parameters():
                if param.dtype != self.config.param_dtype:
                    param.data = param.data.contiguous().to(self.config.param_dtype)
            for name, buf in self.model.named_buffers():
                if buf.dtype != self.config.param_dtype and buf.dtype in [torch.float16, torch.float32]:
                    buf.data = buf.data.contiguous().to(self.config.param_dtype)
            
            logger.success("Wan2.1 æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
            
        except Exception as e:
            logger.error(f"Wan2.1 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _get_mask(self, lat_h, lat_w, frame_num):
        """
        æ„å»º I2V æ©ç  (å‚è€ƒå®˜æ–¹ image2video.py)
        """
        # F = frame_num
        # msk = torch.ones(1, F, lat_h, lat_w, device=self.device)
        # msk[:, 1:] = 0
        # ... logic from reference ...
        
        F = frame_num
        mask = torch.ones(1, F, lat_h, lat_w, device=self.device)
        mask[:, 1:] = 0
        
        # Handle VAE stride (4 in time)
        # Repeat first frame mask 4 times?
        # Official code:
        # msk = torch.concat([torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1), msk[:, 1:]], dim=1)
        # msk = msk.view(1, msk.shape[1] // 4, 4, lat_h, lat_w)
        # msk = msk.transpose(1, 2)[0]
        
        mask = torch.concat([
            torch.repeat_interleave(mask[:, 0:1], repeats=4, dim=1), 
            mask[:, 1:]
        ], dim=1)
        mask = mask.view(1, mask.shape[1] // 4, 4, lat_h, lat_w)
        mask = mask.transpose(1, 2)[0] # [C=4, T_lat, H, W] ? 
        # Wait, mask shape in reference ends up as?
        # VAE encodes to 4 channels? No, VAE latent is 16 channels.
        # Wan2.1 VAE stride is (4, 8, 8).
        # Time stride is 4.
        # Input to VAE is [3, F, H, W]. Output is [16, F/4, H/8, W/8].
        # The mask is constructed in latent space directly? 
        # No, reference code constructs `msk` with `lat_h, lat_w`.
        # `msk` shape: `[1, F, lat_h, lat_w]`.
        # After concat repeats: `[1, F+3, lat_h, lat_w]`.
        # After view: `[1, (F+3)//4, 4, lat_h, lat_w]`.
        # After transpose: `[1, 4, (F+3)//4, lat_h, lat_w]`.
        # [0]: `[4, T_lat, H, W]`.
        # Mask has 4 channels?
        # But `y` (latent) has 16 channels.
        # `y = torch.concat([msk, y])`.
        # So `msk` must have compatible dimensions.
        # If `msk` is 4 channels and `y` is 16, total 20?
        # But `in_dim` is 48.
        # `noise` (16) + `y` (32).
        # So `y` must be 32.
        # `y` from VAE is 16.
        # So `msk` must be 16 channels!
        # In reference code: `msk = msk.transpose(1, 2)[0]`.
        # Does `repeat_interleave` repeats 4 make it 4 channels?
        # No, `repeat_interleave` is on `dim=1` (time).
        # Ah, the reference logic handles the specific VAE structure.
        # Wait, `Wan2.1` VAE output is 16 channels.
        # Is `msk` 16 channels?
        # Let's look closely at reference:
        # msk starts [1, F, h, w].
        # repeats 4 times on dim 1 -> [1, 4, h, w] (for first frame).
        # concat -> [1, F+3, h, w].
        # view -> [1, T_lat, 4, h, w].
        # transpose(1, 2) -> [1, 4, T_lat, h, w].
        # [0] -> [4, T_lat, h, w].
        # So mask is 4 channels.
        # 4 + 16 = 20.
        # Where does 48 come from?
        # 16 (noise) + 16 (cond) + 16 (mask) = 48?
        # Maybe `Wan2.1` 1.3B/14B uses 16 channels, but 5B uses different?
        # Or maybe I misread the reference code.
        # Let's check `WanI2V` init `param_dtype`.
        # Wait, I might be looking at `Wan2.1` reference but the model is `Wan2.2`?
        # The user's web search mentioned `Wan2.2`.
        # But the file `image2video.py` is in `Robbyant/lingbot/wan`, which seems to be the local library.
        
        # Let's re-read `image2video.py` line 302: `msk = torch.ones(1, F, lat_h, lat_w, ...)`
        # `lat_h` is latent height.
        # If `msk` is 4 channels.
        # `y = self.vae.encode(...)`.
        # `y` is 16 channels.
        # `y = torch.concat([msk, y])` -> 20 channels.
        # `noise` is 16 channels.
        # `x` passed to model = `cat(noise, y)` = 16 + 20 = 36 channels.
        # But `in_dim` is 48. 36 != 48.
        # Something is missing.
        # Maybe `msk` logic produces 16 channels?
        # `msk` starts [1, F, ...].
        # `repeats=4`.
        # `view(..., 4, ...)`.
        # It creates 4 channels.
        # Unless `repeat_interleave` repeats 16 times? No, 4.
        
        # Wait, `msk` repeats 4 times.
        # Maybe `lat_h` / `lat_w` are different?
        # `Wan2_1_VAE` has `z_dim=16`.
        
        # Let's check `WanModel` `in_dim` in `image2video.py`.
        # It's not explicitly set in `__init__`, so it comes from `config`.
        # `config.in_dim` for I2V might be 36?
        # But `Wan2.1` paper says 16 channels VAE.
        # If `in_dim` is 48, maybe mask is 16 channels?
        # How to get 16 channels from 1? Repeat 16 times?
        # The reference code repeats 4 times.
        
        # Let's blindly follow the reference code for now, assuming it matches the model.
        # BUT, `config.in_dim` in my `video_func.py` is set to 48.
        # If I produce 36 channels and pass to 48-channel model, it will crash.
        # I need to match `in_dim`.
        
        # Maybe `msk` is repeated 4 times AGAIN?
        # Or `y` (latent) is 32 channels? No, `z_dim=16`.
        
        # Let's look at `msk` construction again.
        # `msk = torch.concat([torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1), msk[:, 1:]], dim=1)`
        # This extends the TIME dimension.
        # `msk = msk.view(1, msk.shape[1] // 4, 4, lat_h, lat_w)`
        # Reshapes time into (T/4, 4).
        # `msk.transpose(1, 2)` -> (1, 4, T/4, H, W).
        # So it puts 4 time steps into channels.
        # This is essentially "patching" the mask in time.
        # If the mask is 4 channels.
        
        # What if `Wan2.1` I2V uses `concat` of [Noise(16), Mask(16), Cond(16)]?
        # If I use `torch.repeat_interleave(..., repeats=16, ...)`?
        
        # Let's check `image2video.py` imports.
        # `from .modules.vae2_1 import Wan2_1_VAE`
        # It uses `Wan2_1_VAE`.
        
        # Maybe `config.in_dim` in `image2video.py` is NOT 48?
        # Let's check `backend/app/engines/Robbyant/lingbot/wan/configs/wan_i2v_A14B.py` if it exists.
        
        pass

    def encode_image(self, image_path: str, height: int, width: int):
        """
        å¯¹å‚è€ƒå›¾è¿›è¡Œç¼–ç  (Wan2.1 VAE)
        """
        if not self.vae:
            logger.error("VAE not initialized")
            return None

        try:
            # 1. åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
            img = Image.open(image_path).convert('RGB')
            # ç®€å• Resize (bicubic)
            img = img.resize((width, height), Image.BICUBIC)
            
            # 2. è½¬ Tensor å¹¶å½’ä¸€åŒ– [-1, 1]
            # [3, H, W]
            img_tensor = transforms.ToTensor()(img).to(self.device)
            img_tensor = (img_tensor - 0.5) * 2.0
            
            # 3. æ„é€ è§†é¢‘è¾“å…¥æ ¼å¼ [1, 3, F, H, W]
            # Wan2.1 VAE éœ€è¦è¾“å…¥è§†é¢‘åºåˆ—
            # å‚è€ƒ image2video.py:
            # input: [3, F, H, W] (batched later)
            # Here we just encode 1 frame? No, we need to pad to temporal window?
            # Reference:
            # torch.concat([
            #     torch.nn.functional.interpolate(img[None].cpu(), size=(h, w), ...).transpose(0, 1),
            #     torch.zeros(3, F - 1, h, w)
            # ], dim=1)
            
            F = self.config.frame_num
            # H, W must be divisible by stride? handled by resize above
            
            # [1, 3, H, W]
            x = img_tensor.unsqueeze(0) 
            # [3, 1, H, W]
            x = x.transpose(0, 1)
            
            # Pad with zeros for remaining frames
            zeros = torch.zeros(3, F - 1, height, width, device=self.device)
            # [3, F, H, W]
            x_seq = torch.cat([x, zeros], dim=1)
            
            # 4. VAE Encode
            # Output: [1, 16, f, h, w]
            with torch.no_grad():
                latent = self.vae.encode([x_seq])[0]
            
            if self.config.in_dim == 48 and isinstance(self.vae, Wan2_2_VAE):
                logger.info(f"Encoded image latent shape: {latent.shape}")
                return latent

            base_repeats = 4
            _, lat_f, lat_h, lat_w = latent.shape
            msk = torch.ones(1, F, lat_h, lat_w, device=self.device)
            msk[:, 1:] = 0
            msk = torch.concat([
                torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1),
                msk[:, 1:]
            ], dim=1)
            t_len = msk.shape[1]
            if t_len % base_repeats != 0:
                pad_len = base_repeats - (t_len % base_repeats)
                msk = torch.cat([msk, torch.zeros(1, pad_len, lat_h, lat_w, device=self.device)], dim=1)
            msk_view = msk.view(1, msk.shape[1] // base_repeats, base_repeats, lat_h, lat_w)
            msk_base = msk_view.transpose(1, 2)[0]
            if msk_base.shape[1] != lat_f:
                msk_base = torch.nn.functional.interpolate(msk_base.unsqueeze(0), size=(lat_f, lat_h, lat_w), mode='nearest')[0]
            msk_final = msk_base
            cond_latent = torch.cat([msk_final, latent], dim=0)
            logger.info(f"Encoded image latent shape: {cond_latent.shape} (Mask: {msk_final.shape}, Latent: {latent.shape})")
            return cond_latent

        except Exception as e:
            logger.error(f"Image encoding failed: {e}")
            return None

    def generate(self, prompt: str, seed: int = -1, steps: int = 20, guide_scale: float = 5.0, model_name: str = "Wan2.1-T2V-1.3B", image_path: str = None):
        """
        ç”Ÿæˆè§†é¢‘
        """
        self.load_models(model_name)

        # 480 * 832 æ¥è¿‘ 16:9 çš„é•¿å®½æ¯” (832/480 = 1.733, 16/9 = 1.777)
        max_area = 480 * 832 
        frame_num = self.config.frame_num
        
        # Latent (æ½œåœ¨ç©ºé—´) è®¡ç®—
        aspect_ratio = 16/9
        lat_h = round(np.sqrt(max_area * aspect_ratio) // self.config.vae_stride[1] // self.config.patch_size[1] * self.config.patch_size[1])
        lat_w = round(np.sqrt(max_area / aspect_ratio) // self.config.vae_stride[2] // self.config.patch_size[2] * self.config.patch_size[2])
        lat_f = (frame_num - 1) // self.config.vae_stride[0] + 1
        max_seq_len = lat_f * lat_h * lat_w // (self.config.patch_size[1] * self.config.patch_size[2])

        logger.info(f"Latent Shape: {lat_f}x{lat_h}x{lat_w}, Max Seq Len: {max_seq_len}")

        # éšæœºç§å­
        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)
        logger.info(f"Using Seed: {seed}")
        seed_g = torch.Generator(device=self.device)
        seed_g.manual_seed(seed)
        
        # åˆå§‹å™ªå£°
        if self.config.in_dim == 48:
            noise_latent = torch.randn(
                48, lat_f, lat_h, lat_w,
                dtype=torch.float32, generator=seed_g, device=self.device
            )

            fixed_cond = None
            fixed_mask = None
            if image_path:
                target_h = lat_h * self.config.vae_stride[1]
                target_w = lat_w * self.config.vae_stride[2]
                logger.info(f"æ­£åœ¨ç¼–ç å‚è€ƒå›¾åƒ: {image_path} ({target_w}x{target_h})...")

                encoded = self.encode_image(image_path, target_h, target_w)
                if encoded is None:
                    logger.warning("å‚è€ƒå›¾ç¼–ç å¤±è´¥ï¼Œé™çº§ä¸º T2V æ¨¡å¼")
                else:
                    fixed_cond = encoded.to(self.device)
                    if fixed_cond.shape[1] != lat_f or fixed_cond.shape[2] != lat_h or fixed_cond.shape[3] != lat_w:
                        fixed_cond = torch.nn.functional.interpolate(
                            fixed_cond.unsqueeze(0),
                            size=(lat_f, lat_h, lat_w),
                            mode='nearest'
                        )[0]
                    fixed_mask = torch.zeros(1, lat_f, lat_h, lat_w, device=self.device)
                    fixed_mask[:, 0] = 1.0
                    fixed_mask = fixed_mask.repeat(fixed_cond.shape[0], 1, 1, 1)
            else:
                logger.info("TI2V æ¨¡å‹æœªæä¾›å›¾ç‰‡ï¼Œä½¿ç”¨çº¯æ–‡ç”Ÿè§†é¢‘æ¨¡å¼")

            noise = noise_latent
        else:
            # T2V æ¨¡å‹ (16é€šé“)
            noise = torch.randn(
                self.config.in_dim,
                lat_f,
                lat_h,
                lat_w,
                dtype=torch.float32,
                generator=seed_g,
                device=self.device)
            
            if image_path:
                logger.warning(f"å½“å‰æ¨¡å‹ {model_name} (in_dim={self.config.in_dim}) ä¸æ”¯æŒ I2Vï¼Œå°†å¿½ç•¥å‚è€ƒå›¾ï¼Œä»…ä½¿ç”¨æç¤ºè¯ç”Ÿæˆã€‚")
        
        n_prompt = self.config.sample_neg_prompt

        # æ–‡æœ¬ç¼–ç 
        logger.info(f"æ­¥éª¤ 1/3: æ­£åœ¨ç¼–ç æç¤ºè¯ ({self.config.text_len} tokens)...")
        # å¦‚æœæ˜¾å­˜ç´§å¼ ï¼Œç¼–ç åå°† T5 å¸è½½åˆ° CPUï¼Œä½†è¿™é‡Œæš‚æ—¶ä¿æŒç®€å•
        # æˆ–è€…æ›´å¥½ï¼šç§»åŠ¨åˆ°è®¾å¤‡ï¼Œç¼–ç ï¼Œç„¶åç§»å›
        self.text_encoder.model.to(self.device)
        # ç¡®ä¿ T5 ä¹Ÿåœ¨æ­£ç¡®çš„æ•°æ®ç±»å‹ä¸‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰ï¼Œå°½ç®¡ T5EncoderModel ä¼šå¤„ç†å®ƒ
        # context = self.text_encoder([prompt], self.device)
        # context_null = self.text_encoder([n_prompt], self.device)
        
        # æ˜¾å¼å¤„ç†ä¸Šä¸‹æ–‡ç”Ÿæˆ
        context = self.text_encoder([prompt], self.device)
        context_null = self.text_encoder([n_prompt], self.device)
        
        # ç¡®ä¿ä¸Šä¸‹æ–‡åœ¨ param_dtype (bf16) ä¸­
        if isinstance(context, list):
            context = [c.to(self.config.param_dtype) for c in context]
        else:
            context = context.to(self.config.param_dtype)
            
        if isinstance(context_null, list):
            context_null = [c.to(self.config.param_dtype) for c in context_null]
        else:
            context_null = context_null.to(self.config.param_dtype)

        self.text_encoder.model.cpu() # å¸è½½ T5
        torch.cuda.empty_cache()

        # è°ƒåº¦å™¨
        sample_scheduler = FlowUniPCMultistepScheduler(
            num_train_timesteps=self.config.num_train_timesteps,
            shift=1,
            use_dynamic_shifting=False)
        sample_scheduler.set_timesteps(steps, device=self.device, shift=5.0)
        timesteps = sample_scheduler.timesteps

        latent = noise
        arg_c = {'context': [context[0]], 'seq_len': max_seq_len}
        arg_null = {'context': context_null, 'seq_len': max_seq_len}

        # é‡‡æ ·å¾ªç¯
        logger.info(f"Step 2/3: å¼€å§‹æ‰©æ•£é‡‡æ · (Total Steps: {steps})...")
        with torch.amp.autocast('cuda', dtype=self.config.param_dtype), torch.no_grad():
            for i, t in enumerate(timesteps):
                if (i + 1) % 5 == 0 or (i + 1) == steps or i == 0:
                    logger.info(f"  é‡‡æ ·è¿›åº¦: {i + 1}/{steps} ({(i + 1) / steps * 100:.0f}%)")
                
                latent_model_input = [latent.to(self.device)]
                timestep = [t]
                timestep = torch.stack(timestep).to(self.device)

                noise_pred_cond = self.model(latent_model_input, t=timestep, **arg_c)[0]
                noise_pred_uncond = self.model(latent_model_input, t=timestep, **arg_null)[0]
                
                noise_pred = noise_pred_uncond + guide_scale * (noise_pred_cond - noise_pred_uncond)
                
                temp_x0 = sample_scheduler.step(
                    noise_pred.unsqueeze(0),
                    t,
                    latent.unsqueeze(0),
                    return_dict=False,
                    generator=seed_g)[0]
                latent = temp_x0.squeeze(0)
                if self.config.in_dim == 48 and fixed_cond is not None and fixed_mask is not None:
                    latent = latent * (1 - fixed_mask) + fixed_cond * fixed_mask

        # è§£ç 
        logger.info("Step 3/3: æ­£åœ¨è§£ç è§†é¢‘ (VAE Decoding)...")
        # self.model.cpu() # å¦‚æœæœ‰éœ€è¦ï¼Œå¯ä»¥å¸è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜
        # torch.cuda.empty_cache()
        
        # å¦‚æœæ˜¯ TI2V æ¨¡å‹ (48é€šé“)ï¼Œåªå–å‰ 16 ä¸ªé€šé“ä½œä¸ºç”Ÿæˆçš„ Latent
        if isinstance(self.vae, Wan2_1_VAE) and latent.shape[0] > 16:
            latent = latent[:16]

        videos = self.vae.decode([latent])
        
        # æ£€æŸ¥ç”Ÿæˆç»“æœç»Ÿè®¡ä¿¡æ¯ (Debug Solid Color Issue)
        video_tensor = videos[0]
        v_min, v_max = video_tensor.min().item(), video_tensor.max().item()
        v_mean, v_std = video_tensor.mean().item(), video_tensor.std().item()
        logger.info(f"Generated Video Stats: Min={v_min:.4f}, Max={v_max:.4f}, Mean={v_mean:.4f}, Std={v_std:.4f}")
        
        if v_std < 1e-3:
            logger.error("âš ï¸ è­¦å‘Š: ç”Ÿæˆçš„è§†é¢‘æ–¹å·®æä½ï¼Œå¯èƒ½æ˜¯çº¯è‰²è§†é¢‘ï¼è¯·æ£€æŸ¥æ¨¡å‹è¾“å…¥æˆ–å‚æ•°ã€‚")
        
        if image_path:
            try:
                frame = video_tensor[:, 0]
                ref_img = Image.open(image_path).convert('RGB')
                ref_img = ref_img.resize((frame.shape[2], frame.shape[1]), Image.BICUBIC)
                ref_tensor = transforms.ToTensor()(ref_img).to(frame.device)
                ref_tensor = (ref_tensor - 0.5) * 2.0
                mse = torch.mean((frame - ref_tensor) ** 2).item()
                mae = torch.mean(torch.abs(frame - ref_tensor)).item()
                f = frame.permute(1, 2, 0).reshape(-1, 3)
                r = ref_tensor.permute(1, 2, 0).reshape(-1, 3)
                f_norm = f / (torch.norm(f, dim=1, keepdim=True) + 1e-8)
                r_norm = r / (torch.norm(r, dim=1, keepdim=True) + 1e-8)
                cos_sim = torch.mean(torch.sum(f_norm * r_norm, dim=1)).item()
                logger.info(f"I2V é¦–å¸§ä¸å‚è€ƒå›¾ç›¸ä¼¼åº¦: Cos={cos_sim:.4f}, MSE={mse:.2f}, MAE={mae:.2f}")
                if cos_sim < 0.85:
                    logger.error("âš ï¸ è­¦å‘Š: I2V é¦–å¸§ä¸å‚è€ƒå›¾ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå¯èƒ½æœªæ­£ç¡®æ³¨å…¥å›¾åƒæ¡ä»¶ã€‚")
            except Exception as e:
                logger.error(f"I2V é¦–å¸§ç›¸ä¼¼åº¦æ£€æµ‹å¤±è´¥: {e}")
        
        return videos[0]


# =============================================================================
# Manager
# =============================================================================

class VideoManager:
    """
    è§†é¢‘ç”Ÿæˆä¸šåŠ¡é€»è¾‘ç®¡ç†å™¨
    """
    
    @staticmethod
    async def preload_model():
        """
        å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ (å¼‚æ­¥æ‰§è¡Œ)
        """
        try:
            logger.info("ğŸš€ [VideoManager] å¼€å§‹é¢„åŠ è½½è§†é¢‘ç”Ÿæˆæ¨¡å‹...")
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè€—æ—¶æ“ä½œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            import asyncio
            wrapper = WanT2VWrapper()
            await asyncio.to_thread(wrapper.load_models)
            logger.success("âœ… [VideoManager] è§†é¢‘ç”Ÿæˆæ¨¡å‹é¢„åŠ è½½å®Œæˆ!")
        except Exception as e:
            logger.error(f"âŒ [VideoManager] æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {e}")

    @staticmethod
    async def generate_video(req: VideoGenRequest) -> VideoGenResponse:
        start_time = time.time()
        task_id = str(uuid.uuid4())
        logger.info(f"å¼€å§‹ç”Ÿæˆè§†é¢‘, æç¤ºè¯: {req.prompt}, ä»»åŠ¡ID: {task_id}")
        
        # 0. åˆ›å»ºæ•°æ®åº“è®°å½• (Pending)
        session_factory = PGUtils.get_session_factory()
        async with session_factory() as session:
            try:
                new_task = AIVideoTask(
                    task_id=task_id,
                    prompt=req.prompt,
                    model=req.model,
                    status="processing", # æ ‡è®°ä¸ºå¤„ç†ä¸­
                    cost_time=0.0
                )
                session.add(new_task)
                await session.commit()
            except Exception as e:
                logger.error(f"åˆ›å»ºæ•°æ®åº“ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
                # å³ä½¿DBå¤±è´¥ï¼Œä¹Ÿå°è¯•ç»§ç»­ç”Ÿæˆï¼Œæˆ–è€…ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                # raise e 
        
        # 0.5 å‘é€é£ä¹¦é€šçŸ¥ (ä»»åŠ¡å¼€å§‹)
        try:
            feishu = FeishuBot()
            start_msg = f"ğŸš€ **è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨**\n\nğŸ†” ä»»åŠ¡ID: {task_id}\nğŸ“ æç¤ºè¯: {req.prompt}\nğŸ¤– æ¨¡å‹: {req.model}\nâ³ çŠ¶æ€: å¤„ç†ä¸­..."
            feishu.send_webhook_message(start_msg)
        except Exception as e:
            logger.error(f"é£ä¹¦å¯åŠ¨é€šçŸ¥å‘é€å¤±è´¥: {e}")
        
        try:
            # 1. ç”Ÿæˆè§†é¢‘
            wrapper = WanT2VWrapper()
            
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ç”Ÿæˆ
            import asyncio
            loop = asyncio.get_running_loop()
            
            video_tensor = await loop.run_in_executor(
                None, 
                wrapper.generate, 
                req.prompt, 
                req.seed, 
                req.sampling_steps, 
                req.guide_scale,
                req.model,
                req.image_path # ä¼ é€’å›¾ç‰‡è·¯å¾„
            )
            
            # 2. ä¿å­˜è§†é¢‘æ–‡ä»¶ (ä¸´æ—¶ä¿å­˜)
            today = time.strftime("%Y%m%d")
            filename = f"{task_id}.mp4"
            # ä¸´æ—¶ç›®å½•
            temp_dir = Path("/tmp/trai_video_gen") 
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_path = temp_dir / filename
            
            # å…ˆä¿å­˜åˆ°æœ¬åœ°
            save_video(
                tensor=video_tensor[None],
                save_file=str(temp_path),
                fps=16,
                nrow=1,
                normalize=True,
                value_range=(-1, 1)
            )
            
            # 3. ä¸Šä¼ åˆ° S3 (æˆ–æœ¬åœ°é™æ€ç›®å½•)
            with open(temp_path, "rb") as f:
                video_bytes = f.read()
            
            # 3.1 ç”Ÿæˆå°é¢å›¾ (å–ç¬¬ä¸€å¸§)
            cover_url = None
            cover_image_key = None
            try:
                import cv2
                cover_filename = f"{task_id}.jpg"
                cover_path = temp_dir / cover_filename
                
                # ä½¿ç”¨ OpenCV æå–ç¬¬ä¸€å¸§
                cap = cv2.VideoCapture(str(temp_path))
                if not cap.isOpened():
                    logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶æå–å°é¢: {temp_path}")
                else:
                    ret, frame = cap.read()
                    if ret:
                        # ä¿å­˜ä¸º JPEG
                        cv2.imwrite(str(cover_path), frame)
                        logger.info(f"å°é¢å›¾æå–æˆåŠŸ: {cover_path}")
                    else:
                        logger.error("æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§")
                    cap.release()
                
                if cover_path.exists():
                    with open(cover_path, "rb") as f:
                        cover_bytes = f.read()
                        
                    # ä¸Šä¼ å°é¢åˆ° S3
                    c_url, _, _ = await UploadUtils.save_from_bytes(
                        data=cover_bytes,
                        filename=cover_filename,
                        module="ai_video/covers",
                        content_type="image/jpeg"
                    )
                    
                    if not c_url.startswith("http"):
                        if not c_url.startswith("/"):
                            c_url = "/" + c_url
                    cover_url = c_url
                    
                    # ä¸Šä¼ å°é¢åˆ°é£ä¹¦ (è·å– image_key)
                    try:
                        feishu = FeishuBot()
                        cover_image_key = feishu.upload_image(cover_bytes)
                    except Exception as e:
                        logger.warning(f"ä¸Šä¼ å°é¢åˆ°é£ä¹¦å¤±è´¥: {e}")
                        
                    # æ¸…ç†å°é¢ä¸´æ—¶æ–‡ä»¶
                    os.remove(cover_path)
            except ImportError:
                logger.error("æœªå®‰è£… opencv-python-headlessï¼Œè·³è¿‡å°é¢æå–")
            except Exception as e:
                logger.error(f"ç”Ÿæˆå°é¢å›¾å¤±è´¥: {e}")
                
            # ä½¿ç”¨ UploadUtils ä¸Šä¼ 
            # save_from_bytes è¿”å› (url_path, file_path, size)
            # module="ai_video"
            video_url, _, _ = await UploadUtils.save_from_bytes(
                data=video_bytes,
                filename=filename,
                module="ai_video",
                content_type="video/mp4"
            )
            
            # å¦‚æœæ˜¯æœ¬åœ°å­˜å‚¨ï¼ŒUploadUtils è¿”å›çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼è£…å®Œæ•´ URL
            if not video_url.startswith("http"):
                # å‡è®¾ API åŸºç¡€ URLï¼Œæˆ–è€…å‰ç«¯é€šè¿‡ /static è®¿é—®
                # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå‡è®¾æ˜¯ /static/...
                if not video_url.startswith("/"):
                    video_url = "/" + video_url
            
            cost_time = time.time() - start_time
            logger.success(f"è§†é¢‘ç”Ÿæˆå¹¶ä¸Šä¼ æˆåŠŸ: {video_url}, è€—æ—¶: {cost_time:.2f}s")
            
            # 4. æ›´æ–°æ•°æ®åº“ (Success)
            async with session_factory() as session:
                async with session.begin():
                    # é‡æ–°æŸ¥è¯¢ä»¥è·å–æœ€æ–°çŠ¶æ€
                    task = await PGUtils.fetch_one(
                        "SELECT * FROM ai_video_tasks WHERE task_id = :task_id", 
                        {"task_id": task_id}
                    )
                    if task:
                        await PGUtils.execute_update(
                            """
                            UPDATE ai_video_tasks 
                            SET status = :status, video_url = :video_url, cover_url = :cover_url, cost_time = :cost_time, updated_at = NOW()
                            WHERE task_id = :task_id
                            """,
                            {
                                "status": "success",
                                "video_url": video_url,
                                "cover_url": cover_url,
                                "cost_time": cost_time,
                                "task_id": task_id
                            }
                        )

            # 5. å‘é€é£ä¹¦é€šçŸ¥ (å¡ç‰‡æ¶ˆæ¯)
            try:
                feishu = FeishuBot()
                
                # æ³¨æ„: é£ä¹¦ Webhook ä¸æ”¯æŒç›´æ¥å‘é€è§†é¢‘æ–‡ä»¶ (media/file ç±»å‹)
                # å› æ­¤æˆ‘ä»¬ä½¿ç”¨äº¤äº’å¼å¡ç‰‡å±•ç¤ºå°é¢å›¾å’Œé“¾æ¥ï¼Œè¿™æ˜¯ Webhook çš„æœ€ä½³å®è·µ
                
                # å°è¯•å‘é€äº¤äº’å¼å¡ç‰‡
                if cover_image_key:
                    card_content = {
                        "config": {
                            "wide_screen_mode": True
                        },
                        "header": {
                            "title": {
                                "tag": "plain_text",
                                "content": "ğŸ¬ è§†é¢‘ç”Ÿæˆå®Œæˆ"
                            },
                            "template": "blue"
                        },
                        "elements": [
                            {
                                "tag": "div",
                                "fields": [
                                    {
                                        "is_short": True,
                                        "text": {
                                            "tag": "lark_md",
                                            "content": f"**ä»»åŠ¡ID**: {task_id}"
                                        }
                                    },
                                    {
                                        "is_short": True,
                                        "text": {
                                            "tag": "lark_md",
                                            "content": f"**è€—æ—¶**: {cost_time:.2f}s"
                                        }
                                    }
                                ]
                            },
                            {
                                "tag": "div",
                                "text": {
                                    "tag": "lark_md",
                                    "content": f"**æç¤ºè¯**: {req.prompt}"
                                }
                            },
                            {
                                "tag": "img",
                                "img_key": cover_image_key,
                                "alt": {
                                    "tag": "plain_text",
                                    "content": "è§†é¢‘å°é¢"
                                },
                                "mode": "crop_center",
                                "custom_width": 300
                            },
                            {
                                "tag": "action",
                                "actions": [
                                    {
                                        "tag": "button",
                                        "text": {
                                            "tag": "plain_text",
                                            "content": "â–¶ï¸ ç‚¹å‡»æ’­æ”¾è§†é¢‘"
                                        },
                                        "url": video_url,
                                        "type": "primary"
                                    },
                                    {
                                        "tag": "button",
                                        "text": {
                                            "tag": "plain_text",
                                            "content": "ğŸ“¥ ä¸‹è½½"
                                        },
                                        "url": video_url,
                                        "type": "default"
                                    }
                                ]
                            }
                        ]
                    }
                    feishu.send_webhook_card(card_content)
                else:
                    # é™çº§ä¸ºæ–‡æœ¬
                    msg = f"ğŸ¬ **è§†é¢‘ç”Ÿæˆå®Œæˆ**\n\nğŸ†” ä»»åŠ¡ID: {task_id}\nğŸ“ æç¤ºè¯: {req.prompt}\nâ±ï¸ è€—æ—¶: {cost_time:.2f}s\nğŸ”— é“¾æ¥: {video_url}"
                    feishu.send_webhook_message(msg)
            except Exception as e:
                logger.error(f"Feishu notification failed: {e}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (ç§»åˆ°æœ€åï¼Œç¡®ä¿ä¸Šä¼ å®Œæˆåæ¸…ç†)
            try:
                if temp_path.exists():
                    os.remove(temp_path)
            except:
                pass

            return VideoGenResponse(
                video_url=video_url,
                cost_time=cost_time
            )

        except Exception as e:
            cost_time = time.time() - start_time
            logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ›´æ–°æ•°æ®åº“ (Failed)
            async with session_factory() as session:
                try:
                    await PGUtils.execute_update(
                        """
                        UPDATE ai_video_tasks 
                        SET status = :status, error_msg = :error_msg, cost_time = :cost_time, updated_at = NOW()
                        WHERE task_id = :task_id
                        """,
                        {
                            "status": "failed",
                            "error_msg": str(e)[:500], # æˆªæ–­é”™è¯¯ä¿¡æ¯
                            "cost_time": cost_time,
                            "task_id": task_id
                        }
                    )
                except Exception as db_e:
                    logger.error(f"æ›´æ–°æ•°æ®åº“å¤±è´¥çŠ¶æ€å‡ºé”™: {db_e}")
            
            raise e
