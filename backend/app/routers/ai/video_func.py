#!/usr/bin/env python
# -*- coding: utf-8 -*-
# æ–‡ä»¶åï¼šbackend/app/routers/ai/video_func.py
# ä½œè€…ï¼šliuhd
# æ—¥æœŸï¼š2026-02-06
# æè¿°ï¼šAI è§†é¢‘ç”Ÿæˆä¸šåŠ¡é€»è¾‘ (Wan2.1-T2V-1.3B)

import os
import sys
import uuid
import time
import torch
import logging
import random
import numpy as np
from pathlib import Path
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from easydict import EasyDict
from safetensors.torch import load_file
from contextlib import asynccontextmanager

# é¡¹ç›®å†…éƒ¨å¼•ç”¨
from backend.app.config import settings
from backend.app.utils.logger import logger
from backend.app.utils.pg_utils import PGUtils
from backend.app.utils.upload_utils import UploadUtils
from backend.app.utils.feishu_utils import FeishuBot
from backend.app.models.ai_video import AIVideoTask

# Wan-AI æ¨¡å—å¼•ç”¨
# å‡è®¾ Wan æ¨¡å—åœ¨ backend/app/engines/Robbyant/lingbot ç›®å½•ä¸‹
WAN_MODULE_PATH = Path("/home/code_dev/trai/backend/app/engines/Robbyant/lingbot")
if str(WAN_MODULE_PATH) not in sys.path:
    sys.path.append(str(WAN_MODULE_PATH))

try:
    from wan.modules.model import WanModel
    from wan.modules.t5 import T5EncoderModel
    from wan.modules.vae2_1 import Wan2_1_VAE
    from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler
    from wan.utils.utils import save_video
except ImportError as e:
    logger.error(f"Wan-AI module import failed: {e}")

# =============================================================================
# Schema å®šä¹‰
# =============================================================================

class VideoGenRequest(BaseModel):
    """
    æ–‡ç”Ÿè§†é¢‘è¯·æ±‚
    """
    prompt: str = Field(..., description="æç¤ºè¯", examples=["ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨è‰åœ°ä¸Šå¥”è·‘"])
    model: str = Field("Wan2.1-T2V-1.3B", description="æ¨¡å‹åç§°", examples=["Wan2.1-T2V-1.3B"])
    ratio: str = Field("16:9", description="å®½é«˜æ¯”", examples=["16:9"])
    duration: int = Field(5, description="è§†é¢‘æ—¶é•¿(ç§’) - å®é™…ä¸Šç”± frame_num å†³å®š, è¿™é‡Œä»…ä½œå‚è€ƒ", examples=[5])
    sampling_steps: int = Field(20, description="é‡‡æ ·æ­¥æ•°", examples=[20])
    guide_scale: float = Field(5.0, description="å¼•å¯¼ç³»æ•°", examples=[5.0])
    seed: int = Field(-1, description="éšæœºç§å­ (-1 è¡¨ç¤ºéšæœº)", examples=[-1])

class VideoGenResponse(BaseModel):
    """
    æ–‡ç”Ÿè§†é¢‘å“åº”
    """
    video_url: str = Field(..., description="è§†é¢‘ URL")
    cover_url: Optional[str] = Field(None, description="å°é¢å›¾ URL")
    cost_time: float = Field(..., description="è€—æ—¶(ç§’)")

# =============================================================================
# WanT2V å°è£…ç±»
# =============================================================================

class WanT2VWrapper:
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(WanT2VWrapper, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def __init__(self):
        if self.initialized:
            return
            
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.checkpoint_dir = '/home/code_dev/trai/backend/app/models/Wan-AI/Wan2.1-T2V-1.3B'
        
        # Config (Hardcoded for 1.3B based on test script)
        self.config = EasyDict()
        self.config.t5_model = 'umt5_xxl'
        self.config.t5_dtype = torch.bfloat16
        self.config.text_len = 512
        self.config.param_dtype = torch.bfloat16
        self.config.num_train_timesteps = 1000
        self.config.sample_fps = 16
        self.config.sample_neg_prompt = 'è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°'
        self.config.frame_num = 81
        self.config.t5_checkpoint = 'models_t5_umt5-xxl-enc-bf16.pth'
        self.config.t5_tokenizer = 'google/umt5-xxl'
        self.config.vae_checkpoint = 'Wan2.1_VAE.pth'
        self.config.vae_stride = (4, 8, 8)
        self.config.patch_size = (1, 2, 2)
        self.config.dim = 1536
        self.config.ffn_dim = 8960
        self.config.freq_dim = 256
        self.config.num_heads = 12
        self.config.num_layers = 30
        self.config.window_size = (-1, -1)
        self.config.qk_norm = True
        self.config.cross_attn_norm = True
        self.config.eps = 1e-6
        self.config.in_dim = 16
        self.config.text_dim = 4096
        self.config.out_dim = 16

        self.text_encoder = None
        self.vae = None
        self.model = None
        
        self.initialized = True
        logger.info("WanT2VWrapper é…ç½®åˆå§‹åŒ–å®Œæˆã€‚")

    def load_models(self):
        """
        åŠ è½½æ¨¡å‹ (Lazy Loading)
        """
        if self.model is not None:
            return

        logger.info("æ­£åœ¨åŠ è½½ Wan2.1 æ¨¡å‹...")
        try:
            # 1. T5 Encoder
            self.text_encoder = T5EncoderModel(
                text_len=self.config.text_len,
                dtype=self.config.t5_dtype,
                device=torch.device('cpu'), # Init on CPU
                checkpoint_path=os.path.join(self.checkpoint_dir, self.config.t5_checkpoint),
                tokenizer_path=os.path.join(self.checkpoint_dir, self.config.t5_tokenizer),
                shard_fn=None,
            )

            # 2. VAE
            self.vae = Wan2_1_VAE(
                vae_pth=os.path.join(self.checkpoint_dir, self.config.vae_checkpoint),
                device=self.device)

            # 3. WanModel
            self.model = WanModel(
                model_type='t2v',
                patch_size=self.config.patch_size,
                text_len=self.config.text_len,
                in_dim=self.config.in_dim,
                dim=self.config.dim,
                ffn_dim=self.config.ffn_dim,
                freq_dim=self.config.freq_dim,
                text_dim=self.config.text_dim,
                out_dim=self.config.out_dim,
                num_heads=self.config.num_heads,
                num_layers=self.config.num_layers,
                window_size=self.config.window_size,
                qk_norm=self.config.qk_norm,
                cross_attn_norm=self.config.cross_attn_norm,
                eps=self.config.eps
            )
            
            # Load weights
            state_dict_path = os.path.join(self.checkpoint_dir, "diffusion_pytorch_model.safetensors")
            if os.path.exists(state_dict_path):
                state_dict = load_file(state_dict_path)
                self.model.load_state_dict(state_dict, strict=False)
            else:
                raise FileNotFoundError(f"Weights not found at {state_dict_path}")

            self.model.eval().requires_grad_(False)
            self.model.to(self.device)
            self.model.to(self.config.param_dtype)
            
            # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°å’ŒBufferï¼Œç¡®ä¿æ— é—æ¼
            for name, param in self.model.named_parameters():
                if param.dtype != self.config.param_dtype:
                    param.data = param.data.to(self.config.param_dtype)
            for name, buf in self.model.named_buffers():
                if buf.dtype != self.config.param_dtype and buf.dtype in [torch.float16, torch.float32]:
                    buf.data = buf.data.to(self.config.param_dtype)
            
            logger.success("Wan2.1 æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
            
        except Exception as e:
            logger.error(f"Wan2.1 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def generate(self, prompt: str, seed: int = -1, steps: int = 20, guide_scale: float = 5.0):
        self.load_models()
        
        # Dimensions (480p recommended for 1.3B)
        # 480 * 832 is close to 16:9 aspect ratio (832/480 = 1.733, 16/9 = 1.777)
        max_area = 480 * 832 
        frame_num = self.config.frame_num
        
        # Latent calculations
        aspect_ratio = 16/9
        lat_h = round(np.sqrt(max_area * aspect_ratio) // self.config.vae_stride[1] // self.config.patch_size[1] * self.config.patch_size[1])
        lat_w = round(np.sqrt(max_area / aspect_ratio) // self.config.vae_stride[2] // self.config.patch_size[2] * self.config.patch_size[2])
        # Recalculate h/w based on latents to match VAE requirements
        # h = lat_h * self.config.vae_stride[1]
        # w = lat_w * self.config.vae_stride[2]
        lat_f = (frame_num - 1) // self.config.vae_stride[0] + 1
        max_seq_len = lat_f * lat_h * lat_w // (self.config.patch_size[1] * self.config.patch_size[2])

        # Seed
        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)
        seed_g = torch.Generator(device=self.device)
        seed_g.manual_seed(seed)
        
        # Initial noise
        noise = torch.randn(
            16,
            lat_f,
            lat_h,
            lat_w,
            dtype=torch.float32,
            generator=seed_g,
            device=self.device)
        
        n_prompt = self.config.sample_neg_prompt

        # Text Encoding
        # Offload T5 to CPU after encoding if memory is tight, but here we keep it simple for now
        # Or better: move to device, encode, move back
        self.text_encoder.model.to(self.device)
        # Ensure T5 is also in correct dtype if possible, though T5EncoderModel handles it
        # context = self.text_encoder([prompt], self.device)
        # context_null = self.text_encoder([n_prompt], self.device)
        
        # Explicitly handling context generation
        context = self.text_encoder([prompt], self.device)
        context_null = self.text_encoder([n_prompt], self.device)
        
        # Ensure context is in param_dtype (bf16)
        if isinstance(context, list):
            context = [c.to(self.config.param_dtype) for c in context]
        else:
            context = context.to(self.config.param_dtype)
            
        if isinstance(context_null, list):
            context_null = [c.to(self.config.param_dtype) for c in context_null]
        else:
            context_null = context_null.to(self.config.param_dtype)

        self.text_encoder.model.cpu() # Offload T5
        torch.cuda.empty_cache()

        # Scheduler
        sample_scheduler = FlowUniPCMultistepScheduler(
            num_train_timesteps=self.config.num_train_timesteps,
            shift=1,
            use_dynamic_shifting=False)
        sample_scheduler.set_timesteps(steps, device=self.device, shift=5.0)
        timesteps = sample_scheduler.timesteps

        latent = noise
        arg_c = {'context': [context[0]], 'seq_len': max_seq_len}
        arg_null = {'context': context_null, 'seq_len': max_seq_len}

        # Sampling Loop
        with torch.amp.autocast('cuda', dtype=self.config.param_dtype), torch.no_grad():
            for _, t in enumerate(timesteps):
                latent_model_input = [latent.to(self.device)]
                timestep = [t]
                timestep = torch.stack(timestep).to(self.device)

                noise_pred_cond = self.model(latent_model_input, t=timestep, **arg_c)[0]
                noise_pred_uncond = self.model(latent_model_input, t=timestep, **arg_null)[0]
                
                noise_pred = noise_pred_uncond + guide_scale * (noise_pred_cond - noise_pred_uncond)
                
                temp_x0 = sample_scheduler.step(
                    noise_pred.unsqueeze(0),
                    t,
                    latent.unsqueeze(0),
                    return_dict=False,
                    generator=seed_g)[0]
                latent = temp_x0.squeeze(0)

        # è§£ç 
        # self.model.cpu() # å¦‚æœæœ‰éœ€è¦ï¼Œå¯ä»¥å¸è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜
        # torch.cuda.empty_cache()
        
        videos = self.vae.decode([latent])
        return videos[0]


# =============================================================================
# Manager
# =============================================================================

class VideoManager:
    """
    è§†é¢‘ç”Ÿæˆä¸šåŠ¡é€»è¾‘ç®¡ç†å™¨
    """
    
    @staticmethod
    async def generate_video(req: VideoGenRequest) -> VideoGenResponse:
        start_time = time.time()
        task_id = str(uuid.uuid4())
        logger.info(f"å¼€å§‹ç”Ÿæˆè§†é¢‘, æç¤ºè¯: {req.prompt}, ä»»åŠ¡ID: {task_id}")
        
        # 0. åˆ›å»ºæ•°æ®åº“è®°å½• (Pending)
        session_factory = PGUtils.get_session_factory()
        async with session_factory() as session:
            try:
                new_task = AIVideoTask(
                    task_id=task_id,
                    prompt=req.prompt,
                    model=req.model,
                    status="processing", # æ ‡è®°ä¸ºå¤„ç†ä¸­
                    cost_time=0.0
                )
                session.add(new_task)
                await session.commit()
            except Exception as e:
                logger.error(f"åˆ›å»ºæ•°æ®åº“ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
                # å³ä½¿DBå¤±è´¥ï¼Œä¹Ÿå°è¯•ç»§ç»­ç”Ÿæˆï¼Œæˆ–è€…ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                # raise e 
        
        # 0.5 å‘é€é£ä¹¦é€šçŸ¥ (ä»»åŠ¡å¼€å§‹)
        try:
            feishu = FeishuBot()
            start_msg = f"ğŸš€ **è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨**\n\nğŸ†” ä»»åŠ¡ID: {task_id}\nğŸ“ æç¤ºè¯: {req.prompt}\nğŸ¤– æ¨¡å‹: {req.model}\nâ³ çŠ¶æ€: å¤„ç†ä¸­..."
            feishu.send_webhook_message(start_msg)
        except Exception as e:
            logger.error(f"é£ä¹¦å¯åŠ¨é€šçŸ¥å‘é€å¤±è´¥: {e}")
        
        try:
            # 1. ç”Ÿæˆè§†é¢‘
            wrapper = WanT2VWrapper()
            
            # Run synchronous generation in thread pool
            import asyncio
            loop = asyncio.get_running_loop()
            
            video_tensor = await loop.run_in_executor(
                None, 
                wrapper.generate, 
                req.prompt, 
                req.seed, 
                req.sampling_steps, 
                req.guide_scale
            )
            
            # 2. ä¿å­˜è§†é¢‘æ–‡ä»¶ (ä¸´æ—¶ä¿å­˜)
            today = time.strftime("%Y%m%d")
            filename = f"{task_id}.mp4"
            # ä¸´æ—¶ç›®å½•
            temp_dir = Path("/tmp/trai_video_gen") 
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_path = temp_dir / filename
            
            # Save video locally first
            save_video(
                tensor=video_tensor[None],
                save_file=str(temp_path),
                fps=16,
                nrow=1,
                normalize=True,
                value_range=(-1, 1)
            )
            
            # 3. ä¸Šä¼ åˆ° S3 (æˆ–æœ¬åœ°é™æ€ç›®å½•)
            with open(temp_path, "rb") as f:
                video_bytes = f.read()
            
            # 3.1 ç”Ÿæˆå°é¢å›¾ (å–ç¬¬ä¸€å¸§)
            cover_url = None
            cover_image_key = None
            try:
                import cv2
                cover_filename = f"{task_id}.jpg"
                cover_path = temp_dir / cover_filename
                
                # ä½¿ç”¨ OpenCV æå–ç¬¬ä¸€å¸§
                cap = cv2.VideoCapture(str(temp_path))
                if not cap.isOpened():
                    logger.error(f"Failed to open video file for cover extraction: {temp_path}")
                else:
                    ret, frame = cap.read()
                    if ret:
                        # ä¿å­˜ä¸º JPEG
                        cv2.imwrite(str(cover_path), frame)
                        logger.info(f"Cover image extracted successfully: {cover_path}")
                    else:
                        logger.error("Failed to read first frame from video")
                    cap.release()
                
                if cover_path.exists():
                    with open(cover_path, "rb") as f:
                        cover_bytes = f.read()
                        
                    # ä¸Šä¼ å°é¢åˆ° S3
                    c_url, _, _ = await UploadUtils.save_from_bytes(
                        data=cover_bytes,
                        filename=cover_filename,
                        module="ai_video/covers",
                        content_type="image/jpeg"
                    )
                    
                    if not c_url.startswith("http"):
                        if not c_url.startswith("/"):
                            c_url = "/" + c_url
                    cover_url = c_url
                    
                    # ä¸Šä¼ å°é¢åˆ°é£ä¹¦ (è·å– image_key)
                    try:
                        feishu = FeishuBot()
                        cover_image_key = feishu.upload_image(cover_bytes)
                    except Exception as e:
                        logger.warning(f"Failed to upload cover to Feishu: {e}")
                        
                    # æ¸…ç†å°é¢ä¸´æ—¶æ–‡ä»¶
                    os.remove(cover_path)
            except ImportError:
                logger.error("opencv-python-headless not installed, skipping cover extraction")
            except Exception as e:
                logger.error(f"Failed to generate cover image: {e}")
                
            # ä½¿ç”¨ UploadUtils ä¸Šä¼ 
            # save_from_bytes è¿”å› (url_path, file_path, size)
            # module="ai_video"
            video_url, _, _ = await UploadUtils.save_from_bytes(
                data=video_bytes,
                filename=filename,
                module="ai_video",
                content_type="video/mp4"
            )
            
            # å¦‚æœæ˜¯æœ¬åœ°å­˜å‚¨ï¼ŒUploadUtils è¿”å›çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼è£…å®Œæ•´ URL
            if not video_url.startswith("http"):
                # å‡è®¾ API åŸºç¡€ URLï¼Œæˆ–è€…å‰ç«¯é€šè¿‡ /static è®¿é—®
                # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå‡è®¾æ˜¯ /static/...
                if not video_url.startswith("/"):
                    video_url = "/" + video_url
            
            cost_time = time.time() - start_time
            logger.success(f"è§†é¢‘ç”Ÿæˆå¹¶ä¸Šä¼ æˆåŠŸ: {video_url}, è€—æ—¶: {cost_time:.2f}s")
            
            # 4. æ›´æ–°æ•°æ®åº“ (Success)
            async with session_factory() as session:
                async with session.begin():
                    # é‡æ–°æŸ¥è¯¢ä»¥è·å–æœ€æ–°çŠ¶æ€
                    task = await PGUtils.fetch_one(
                        "SELECT * FROM ai_video_tasks WHERE task_id = :task_id", 
                        {"task_id": task_id}
                    )
                    if task:
                        await PGUtils.execute_update(
                            """
                            UPDATE ai_video_tasks 
                            SET status = :status, video_url = :video_url, cover_url = :cover_url, cost_time = :cost_time, updated_at = NOW()
                            WHERE task_id = :task_id
                            """,
                            {
                                "status": "success",
                                "video_url": video_url,
                                "cover_url": cover_url,
                                "cost_time": cost_time,
                                "task_id": task_id
                            }
                        )

            # 5. å‘é€é£ä¹¦é€šçŸ¥ (å¡ç‰‡æ¶ˆæ¯)
            try:
                feishu = FeishuBot()
                
                # æ³¨æ„: é£ä¹¦ Webhook ä¸æ”¯æŒç›´æ¥å‘é€è§†é¢‘æ–‡ä»¶ (media/file ç±»å‹)
                # å› æ­¤æˆ‘ä»¬ä½¿ç”¨äº¤äº’å¼å¡ç‰‡å±•ç¤ºå°é¢å›¾å’Œé“¾æ¥ï¼Œè¿™æ˜¯ Webhook çš„æœ€ä½³å®è·µ
                
                # å°è¯•å‘é€äº¤äº’å¼å¡ç‰‡
                if cover_image_key:
                    card_content = {
                        "config": {
                            "wide_screen_mode": True
                        },
                        "header": {
                            "title": {
                                "tag": "plain_text",
                                "content": "ğŸ¬ è§†é¢‘ç”Ÿæˆå®Œæˆ"
                            },
                            "template": "blue"
                        },
                        "elements": [
                            {
                                "tag": "div",
                                "fields": [
                                    {
                                        "is_short": True,
                                        "text": {
                                            "tag": "lark_md",
                                            "content": f"**ä»»åŠ¡ID**: {task_id}"
                                        }
                                    },
                                    {
                                        "is_short": True,
                                        "text": {
                                            "tag": "lark_md",
                                            "content": f"**è€—æ—¶**: {cost_time:.2f}s"
                                        }
                                    }
                                ]
                            },
                            {
                                "tag": "div",
                                "text": {
                                    "tag": "lark_md",
                                    "content": f"**æç¤ºè¯**: {req.prompt}"
                                }
                            },
                            {
                                "tag": "img",
                                "img_key": cover_image_key,
                                "alt": {
                                    "tag": "plain_text",
                                    "content": "è§†é¢‘å°é¢"
                                },
                                "mode": "crop_center",
                                "custom_width": 300
                            },
                            {
                                "tag": "action",
                                "actions": [
                                    {
                                        "tag": "button",
                                        "text": {
                                            "tag": "plain_text",
                                            "content": "â–¶ï¸ ç‚¹å‡»æ’­æ”¾è§†é¢‘"
                                        },
                                        "url": video_url,
                                        "type": "primary"
                                    },
                                    {
                                        "tag": "button",
                                        "text": {
                                            "tag": "plain_text",
                                            "content": "ğŸ“¥ ä¸‹è½½"
                                        },
                                        "url": video_url,
                                        "type": "default"
                                    }
                                ]
                            }
                        ]
                    }
                    feishu.send_webhook_card(card_content)
                else:
                    # é™çº§ä¸ºæ–‡æœ¬
                    msg = f"ğŸ¬ **è§†é¢‘ç”Ÿæˆå®Œæˆ**\n\nğŸ†” ä»»åŠ¡ID: {task_id}\nğŸ“ æç¤ºè¯: {req.prompt}\nâ±ï¸ è€—æ—¶: {cost_time:.2f}s\nğŸ”— é“¾æ¥: {video_url}"
                    feishu.send_webhook_message(msg)
            except Exception as e:
                logger.error(f"Feishu notification failed: {e}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (ç§»åˆ°æœ€åï¼Œç¡®ä¿ä¸Šä¼ å®Œæˆåæ¸…ç†)
            try:
                if temp_path.exists():
                    os.remove(temp_path)
            except:
                pass

            return VideoGenResponse(
                video_url=video_url,
                cost_time=cost_time
            )

        except Exception as e:
            cost_time = time.time() - start_time
            logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ›´æ–°æ•°æ®åº“ (Failed)
            async with session_factory() as session:
                try:
                    await PGUtils.execute_update(
                        """
                        UPDATE ai_video_tasks 
                        SET status = :status, error_msg = :error_msg, cost_time = :cost_time, updated_at = NOW()
                        WHERE task_id = :task_id
                        """,
                        {
                            "status": "failed",
                            "error_msg": str(e)[:500], # æˆªæ–­é”™è¯¯ä¿¡æ¯
                            "cost_time": cost_time,
                            "task_id": task_id
                        }
                    )
                except Exception as db_e:
                    logger.error(f"æ›´æ–°æ•°æ®åº“å¤±è´¥çŠ¶æ€å‡ºé”™: {db_e}")
            
            raise e
