#!/usr/bin/env python
# -*- coding: utf-8 -*-
# æ–‡ä»¶åï¼šbackend/app/routers/ai/image_func.py
# ä½œè€…ï¼šliuhd
# æ—¥æœŸï¼š2026-01-28
# æè¿°ï¼šAI å›¾åƒå¤„ç†ä¸å¤šæ¨¡æ€ä¸šåŠ¡é€»è¾‘

import httpx
import os
import time
import uuid
from pathlib import Path
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Union
from backend.app.config import settings
from backend.app.utils.logger import logger
from backend.app.utils.modelscope_utils import ModelScopeUtils

# å…¨å±€ç¼“å­˜æ¨¡å‹ pipeline
_z_image_pipeline = None


# =============================================================================
# Schema å®šä¹‰ (Image/Multimodal)
# =============================================================================

class ImageContent(BaseModel):
    """
    å¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹
    """
    type: str = Field(..., description="ç±»å‹ (text/image)")
    text: Optional[str] = Field(None, description="æ–‡æœ¬å†…å®¹")
    image: Optional[str] = Field(None, description="å›¾ç‰‡é“¾æ¥æˆ–Base64") # æ”¹å image ä»¥åŒ¹é… Qwen æ ¼å¼

class MultimodalMessage(BaseModel):
    """
    å¤šæ¨¡æ€å¯¹è¯æ¶ˆæ¯
    """
    role: str = Field(..., description="è§’è‰² (user/assistant/system)")
    content: List[Dict[str, Any]] = Field(..., description="æ¶ˆæ¯å†…å®¹ (æ”¯æŒçº¯æ–‡æœ¬æˆ–å¤šæ¨¡æ€åˆ—è¡¨)")

class ImageChatRequest(BaseModel):
    """
    AI å›¾åƒå¯¹è¯è¯·æ±‚ (Qwen-VL ç­‰)
    """
    messages: List[MultimodalMessage] = Field(..., description="å†å²æ¶ˆæ¯åˆ—è¡¨")
    model: str = Field("Qwen3-VL-4B-Instruct", description="æ¨¡å‹åç§°")
    temperature: float = Field(0.7, description="æ¸©åº¦ç³»æ•°")
    max_tokens: int = Field(512, description="æœ€å¤§ç”Ÿæˆ Token æ•°")

class ImageChatResponse(BaseModel):
    """
    AI å›¾åƒå¯¹è¯å“åº”
    """
    reply: str = Field(..., description="AI å›å¤å†…å®¹")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    usage: Dict[str, Any] = Field({}, description="Token ä½¿ç”¨ç»Ÿè®¡")

class ImageGenRequest(BaseModel):
    """
    æ–‡ç”Ÿå›¾è¯·æ±‚
    """
    prompt: str = Field(..., description="æç¤ºè¯")
    model: str = Field("FLUX.2-dev", description="æ¨¡å‹åç§°")
    size: str = Field("1024x1024", description="å›¾ç‰‡å°ºå¯¸")
    n: int = Field(1, description="ç”Ÿæˆæ•°é‡")

class ImageGenResponse(BaseModel):
    """
    æ–‡ç”Ÿå›¾å“åº”
    """
    created: int = Field(..., description="åˆ›å»ºæ—¶é—´æˆ³")
    data: List[Dict[str, str]] = Field(..., description="å›¾ç‰‡æ•°æ®åˆ—è¡¨ [{'url': '...'}, ...]")

# =============================================================================
# Manager å®ç°
# =============================================================================

class ImageManager:
    """
    AI å›¾åƒ/å¤šæ¨¡æ€ä¸šåŠ¡ç®¡ç†å™¨
    """
    
    @staticmethod
    async def chat_with_image(request: ImageChatRequest) -> ImageChatResponse:
        """
        å¤šæ¨¡æ€å¯¹è¯ (Qwen-VL) - æœ¬åœ°æ¨ç†
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ (å¦‚æœéœ€è¦é€‚é…å‰ç«¯æ ¼å¼åˆ° Qwen æ ¼å¼)
            # å‡è®¾å‰ç«¯ä¼ æ¥çš„æ ¼å¼å·²ç»æ˜¯:
            # content: [
            #    {"type": "image", "image": "http://..."},
            #    {"type": "text", "text": "æè¿°å›¾ç‰‡"}
            # ]
            # è¿™ä¸ QwenVLUtils æœŸæœ›çš„æ ¼å¼ä¸€è‡´ï¼Œç›´æ¥é€ä¼ 
            
            messages = [msg.model_dump() for msg in request.messages]
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¦æ±‚ä¸­æ–‡å›å¤ (å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®æŒ‡å®šè¯­è¨€)
            # æˆ–è€…åœ¨æœ€åä¸€æ¡æ¶ˆæ¯ä¸­è¿½åŠ æç¤º
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ç”¨æˆ·ä¼šåœ¨ prompt é‡Œé—®ï¼Œæˆ–è€…æˆ‘ä»¬é»˜è®¤è¿½åŠ 
            # è¿™é‡Œä¸å¼ºåˆ¶ä¿®æ”¹ promptï¼Œä»¥å…å½±å“ç”¨æˆ·æ„å›¾
            
            reply = await ModelScopeUtils.chat_completion(
                messages=messages,
                model_name="Qwen3-VL-4B-Instruct",
                max_new_tokens=request.max_tokens
            )
            
            return ImageChatResponse(
                reply=reply,
                model="Qwen3-VL-4B-Instruct",
                usage={"prompt_tokens": 0, "completion_tokens": 0} # æš‚æ— æ³•ç²¾ç¡®ç»Ÿè®¡
            )
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€å¯¹è¯å¤±è´¥: {e}")
            raise ValueError(f"Multimodal chat failed: {e}")

    @staticmethod
    async def generate_image(request: ImageGenRequest) -> ImageGenResponse:
        # ... (ä¿æŒåŸæœ‰çš„æ–‡ç”Ÿå›¾é€»è¾‘æˆ–å¾…å®ç°)
        return ImageGenResponse(created=int(time.time()), data=[])
        api_key = settings.AI_API_KEY or "sk-xxx"

        # é€‚é…æœ¬åœ°æ¨¡å‹æœåŠ¡ (é€šå¸¸ä¸å¸¦ /v1)
        # å¦‚æœé…ç½®ä¸­æœ‰ /v1 ä½†æˆ‘ä»¬éœ€è¦å»æ‰å®ƒ (æ ¹æ®æµ‹è¯•ç»“æœ)
        # ç®€å•å¤„ç†ï¼šå¦‚æœ api_base åŒ…å« /v1ï¼Œå…ˆå°è¯•å»æ‰å®ƒ
        
        base_url = api_base
        if "/v1" in base_url:
            base_url = base_url.replace("/v1", "")
        if base_url.endswith("/"):
            base_url = base_url[:-1]
            
        url = f"{base_url}/chat/completions"

        logger.info(f"æ­£åœ¨è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹: {request.model}, URL: {url}")
        
        try:
            async with httpx.AsyncClient(timeout=100.0) as client:
                response = await client.post(
                    url,
                    json=payload,
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                if response.status_code != 200:
                    logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {response.text}")
                    raise Exception(f"Model API Error: {response.status_code} - {response.text}")
                
                try:
                    data = response.json()
                except Exception:
                    logger.error(f"å“åº”è§£æå¤±è´¥: {response.text[:200]}")
                    raise Exception(f"Invalid JSON response: {response.text[:200]}")
                
                return ImageChatResponse(
                    reply=data["choices"][0]["message"]["content"],
                    model=data["model"],
                    usage=data.get("usage", {})
                )
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€å¯¹è¯å¼‚å¸¸: {str(e)}")
            raise e

    @staticmethod
    async def generate_image(request: ImageGenRequest) -> ImageGenResponse:
        """
        æ–‡ç”Ÿå›¾ (FLUX / Z-Image ç­‰)
        """
        # å¦‚æœè¯·æ±‚æŒ‡å®š Z-Image æ¨¡å‹ï¼Œèµ°æœ¬åœ°è°ƒç”¨
        if "Z-Image" in request.model or "Tongyi-MAI" in request.model:
             return await ImageManager._generate_z_image_local(request)

        # æ„é€ è¯·æ±‚ä½“
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "size": request.size,
            "n": request.n
        }
        
        api_base = settings.DIFY_API_BASE_URL
        api_key = settings.AI_API_KEY
        
        url = f"{api_base}/images/generations"
        # ç±»ä¼¼ chatï¼Œå°è¯•é€‚é…è·¯å¾„
        base_url = api_base
        if "/v1" in base_url:
            base_url = base_url.replace("/v1", "")
        if base_url.endswith("/"):
            base_url = base_url[:-1]
        
        url = f"{base_url}/images/generations"

        logger.info(f"æ­£åœ¨è°ƒç”¨æ–‡ç”Ÿå›¾æ¨¡å‹: {request.model}, URL: {url}")

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    url,
                    json=payload,
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                if response.status_code != 200:
                    logger.error(f"ç”Ÿå›¾å¤±è´¥: {response.text}")
                    raise Exception(f"Image Gen API Error: {response.status_code} - {response.text}")
                
                try:
                    data = response.json()
                except Exception:
                    logger.error(f"å“åº”è§£æå¤±è´¥: {response.text[:200]}")
                    raise Exception(f"Invalid JSON response: {response.text[:200]}")
                
                return ImageGenResponse(
                    created=data.get("created", 0),
                    data=data.get("data", [])
                )
        except Exception as e:
            logger.error(f"æ–‡ç”Ÿå›¾å¼‚å¸¸: {str(e)}")
            raise e

    @staticmethod
    async def _generate_z_image_local(request: ImageGenRequest) -> ImageGenResponse:
        """
        æœ¬åœ°è¿è¡Œ Z-Image æ¨¡å‹ (å¼‚æ­¥åŒ…è£…)
        """
        import asyncio
        loop = asyncio.get_running_loop()
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œé˜»å¡çš„ GPU æ¨ç†ä»£ç 
        return await loop.run_in_executor(None, ImageManager._run_z_image_sync, request)

    @staticmethod
    def _run_z_image_sync(request: ImageGenRequest) -> ImageGenResponse:
        """
        Z-Image åŒæ­¥æ¨ç†é€»è¾‘
        """
        global _z_image_pipeline
        import torch
        from diffusers import ZImagePipeline

        # 1. ç¡®å®šæ¨¡å‹è·¯å¾„
        # backend/app/routers/ai/image_func.py -> backend
        base_dir = Path(__file__).resolve().parent.parent.parent.parent
        # ä½¿ç”¨ app/models
        model_dir = base_dir / "app/models/Tongyi-MAI"
        
        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (å¦‚æœä¸å­˜åœ¨)
        if not model_dir.exists() or not any(model_dir.iterdir()):
            logger.info(f"ğŸ“¥ Z-Image-Turbo æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½: Tongyi-MAI/Z-Image-Turbo -> {model_dir}")
            try:
                from modelscope.hub.snapshot_download import snapshot_download
                snapshot_download("Tongyi-MAI/Z-Image-Turbo", cache_dir=str(base_dir / "app/models"))
                logger.success(f"âœ… Z-Image-Turbo æ¨¡å‹ä¸‹è½½å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ Z-Image-Turbo æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
                raise e
        
        # modelscope ä¸‹è½½åé€šå¸¸ä¼šåœ¨ cache_dir ä¸‹åˆ›å»º Tongyi-MAI/Z-Image-Turbo ç›®å½•
        # æˆ‘ä»¬ä¸Šé¢æŒ‡å®š cache_dir=app/modelsï¼Œæ‰€ä»¥æœ€ç»ˆè·¯å¾„åº”è¯¥æ˜¯ app/models/Tongyi-MAI/Z-Image-Turbo
        # ä½†æˆ‘ä»¬åŸæœ¬çš„é€»è¾‘æ˜¯æŒ‡å‘ Tongyi-MAIï¼Œè¿™é‡Œéœ€è¦é€‚é…ä¸€ä¸‹è·¯å¾„
        # æ£€æŸ¥å®é™…è·¯å¾„
        actual_model_path = base_dir / "app/models/Tongyi-MAI/Z-Image-Turbo"
        if actual_model_path.exists():
            model_path = actual_model_path
        else:
            # å¯èƒ½æ˜¯ç›´æ¥ä¸‹è½½åˆ°äº† Tongyi-MAI (å–å†³äº modelscope ç‰ˆæœ¬ behaviorï¼Œé€šå¸¸æ˜¯ Organization/ModelName)
            model_path = model_dir

        # 2. åŠ è½½æ¨¡å‹ (å•ä¾‹ç¼“å­˜)
        if _z_image_pipeline is None:
             logger.info(f"Loading Z-Image model from {model_path}...")
             try:
                 dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16
                 _z_image_pipeline = ZImagePipeline.from_pretrained(
                     str(model_path),
                     torch_dtype=dtype,
                     low_cpu_mem_usage=False
                 )
                 if torch.cuda.is_available():
                     _z_image_pipeline.to("cuda")
                 logger.success("Z-Image model loaded successfully.")
             except Exception as e:
                 logger.error(f"Failed to load Z-Image model: {e}")
                 raise e

        # 3. ç”Ÿæˆå›¾ç‰‡
        images_data = []
        static_dir = base_dir / "static/gen"
        static_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Start generating {request.n} images with prompt: {request.prompt[:50]}...")
        
        for i in range(request.n):
            # è§£æå°ºå¯¸ (é»˜è®¤ 1024x1024)
            width, height = 1024, 1024
            if "x" in request.size:
                try:
                    w, h = request.size.split("x")
                    width, height = int(w), int(h)
                except:
                    pass

            image = _z_image_pipeline(
                prompt=request.prompt,
                height=height,
                width=width,
                num_inference_steps=9,  # Turbo model recommended steps
                guidance_scale=0.0,     # Turbo model recommended guidance
                generator=torch.Generator("cuda" if torch.cuda.is_available() else "cpu").manual_seed(int(time.time() * 1000) % 2**32),
            ).images[0]
            
            # 4. ä¿å­˜æ–‡ä»¶
            filename = f"z_image_{uuid.uuid4()}.png"
            file_path = static_dir / filename
            image.save(file_path)
            
            # æ„é€ è®¿é—® URL
            # å‡è®¾å‰ç«¯å¯ä»¥é€šè¿‡ /static è®¿é—®
            url = f"/static/gen/{filename}"
            images_data.append({"url": url})
            
            logger.info(f"Generated image: {file_path}")

        return ImageGenResponse(
            created=int(time.time()),
            data=images_data
        )

