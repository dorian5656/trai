# LLM å­¦ä¹ ç¬”è®°ï¼šLlama 3.1 8B Instruct éƒ¨ç½²

æœ¬é¡¹ç›®æ˜¯åŸºäº Datawhale `self-llm` æ•™ç¨‹çš„å­¦ä¹ å®è·µã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

- `download_model.py`: ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹è„šæœ¬
- `main.py`: åŸºäº FastAPI çš„æ¨ç†æœåŠ¡

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

è¯·ç¡®ä¿å·²æ¿€æ´»é¡¹ç›®çš„ conda ç¯å¢ƒ (`trai_31014_whf_pro_20260202`)ã€‚

éœ€è¦å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install modelscope transformers accelerate fastapi uvicorn
```

*æ³¨æ„ï¼š`requirements.txt` å¯èƒ½å·²åŒ…å«éƒ¨åˆ†ä¾èµ–ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè¡¥å……å®‰è£…ã€‚*

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸‹è½½æ¨¡å‹

æ‰§è¡Œä¸‹è½½è„šæœ¬ï¼Œæ¨¡å‹å°†ä¿å­˜è‡³ `backend/app/models/LLM-Research/Meta-Llama-3.1-8B-Instruct`ã€‚

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œ
python backend/app/llm_study/llama3_1/download_model.py
```

**æ³¨æ„**: æ¨¡å‹å¤§å°çº¦ 15GBï¼Œè¯·ç¡®ä¿ç£ç›˜ç©ºé—´å……è¶³ã€‚

### 2. å¯åŠ¨ API æœåŠ¡

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œ
python backend/app/llm_study/llama3_1/main.py
```

æœåŠ¡å°†å¯åŠ¨åœ¨ `http://0.0.0.0:6006`ã€‚

### 3. è°ƒç”¨æµ‹è¯•

å¯ä»¥ä½¿ç”¨ curl è¿›è¡Œæµ‹è¯•ï¼š

```bash
curl -X POST "http://127.0.0.1:6006" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}'
```

## ğŸ”— å‚è€ƒèµ„æ–™

- [Datawhale self-llm Llama3.1 æ•™ç¨‹](https://github.com/datawhalechina/self-llm/blob/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md)
